{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ac9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "307bd652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (make sure to set the path)\n",
    "df = pd.read_csv(\"vanderplanki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7386e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>D0</th>\n",
       "      <th>D24</th>\n",
       "      <th>D48</th>\n",
       "      <th>R3</th>\n",
       "      <th>R24</th>\n",
       "      <th>gene_length</th>\n",
       "      <th>ExonN</th>\n",
       "      <th>Paralogs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.049831</td>\n",
       "      <td>3.494151</td>\n",
       "      <td>1.836840</td>\n",
       "      <td>5.063169</td>\n",
       "      <td>3.128757</td>\n",
       "      <td>1259</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.096369</td>\n",
       "      <td>1.643436</td>\n",
       "      <td>-0.321464</td>\n",
       "      <td>0.736070</td>\n",
       "      <td>1.614264</td>\n",
       "      <td>4529</td>\n",
       "      <td>6</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.933029</td>\n",
       "      <td>4.343837</td>\n",
       "      <td>5.170270</td>\n",
       "      <td>4.424378</td>\n",
       "      <td>3.815149</td>\n",
       "      <td>5586</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.639756</td>\n",
       "      <td>-0.656008</td>\n",
       "      <td>-0.877510</td>\n",
       "      <td>-1.075263</td>\n",
       "      <td>1.011351</td>\n",
       "      <td>8920</td>\n",
       "      <td>9</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.459983</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>-0.330813</td>\n",
       "      <td>-0.259558</td>\n",
       "      <td>2.552977</td>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14996</td>\n",
       "      <td>6.717333</td>\n",
       "      <td>6.696644</td>\n",
       "      <td>6.471900</td>\n",
       "      <td>5.506727</td>\n",
       "      <td>6.748680</td>\n",
       "      <td>549</td>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14997</td>\n",
       "      <td>1.540916</td>\n",
       "      <td>2.296347</td>\n",
       "      <td>1.841658</td>\n",
       "      <td>2.231631</td>\n",
       "      <td>2.020119</td>\n",
       "      <td>1005</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14998</td>\n",
       "      <td>5.134944</td>\n",
       "      <td>5.503081</td>\n",
       "      <td>5.587465</td>\n",
       "      <td>4.704347</td>\n",
       "      <td>5.118015</td>\n",
       "      <td>3058</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14999</td>\n",
       "      <td>7.427295</td>\n",
       "      <td>7.020060</td>\n",
       "      <td>4.781728</td>\n",
       "      <td>5.449490</td>\n",
       "      <td>6.767185</td>\n",
       "      <td>2657</td>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>15000</td>\n",
       "      <td>2.017139</td>\n",
       "      <td>2.044246</td>\n",
       "      <td>1.335987</td>\n",
       "      <td>-0.310788</td>\n",
       "      <td>1.824322</td>\n",
       "      <td>1509</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        D0       D24       D48        R3       R24  \\\n",
       "0               1  3.049831  3.494151  1.836840  5.063169  3.128757   \n",
       "1               2  1.096369  1.643436 -0.321464  0.736070  1.614264   \n",
       "2               3  3.933029  4.343837  5.170270  4.424378  3.815149   \n",
       "3               4  0.639756 -0.656008 -0.877510 -1.075263  1.011351   \n",
       "4               5  2.459983 -0.039614 -0.330813 -0.259558  2.552977   \n",
       "...           ...       ...       ...       ...       ...       ...   \n",
       "14995       14996  6.717333  6.696644  6.471900  5.506727  6.748680   \n",
       "14996       14997  1.540916  2.296347  1.841658  2.231631  2.020119   \n",
       "14997       14998  5.134944  5.503081  5.587465  4.704347  5.118015   \n",
       "14998       14999  7.427295  7.020060  4.781728  5.449490  6.767185   \n",
       "14999       15000  2.017139  2.044246  1.335987 -0.310788  1.824322   \n",
       "\n",
       "       gene_length  ExonN Paralogs  \n",
       "0             1259      2        O  \n",
       "1             4529      6        P  \n",
       "2             5586     11        O  \n",
       "3             8920      9        O  \n",
       "4              862      4        P  \n",
       "...            ...    ...      ...  \n",
       "14995          549      4        O  \n",
       "14996         1005      2        O  \n",
       "14997         3058     10        O  \n",
       "14998         2657      6        O  \n",
       "14999         1509      3        O  \n",
       "\n",
       "[15000 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35b4a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ID from the dataset\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86d7f042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_length</th>\n",
       "      <th>ExonN</th>\n",
       "      <th>Paralogs</th>\n",
       "      <th>expression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3363.834600</td>\n",
       "      <td>5.325267</td>\n",
       "      <td>0.441133</td>\n",
       "      <td>2.496063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5607.723995</td>\n",
       "      <td>6.467899</td>\n",
       "      <td>0.496539</td>\n",
       "      <td>2.822133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.735916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>973.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1738.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.699739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3289.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.372577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>98112.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.986027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gene_length         ExonN      Paralogs    expression\n",
       "count  15000.000000  15000.000000  15000.000000  15000.000000\n",
       "mean    3363.834600      5.325267      0.441133      2.496063\n",
       "std     5607.723995      6.467899      0.496539      2.822133\n",
       "min      160.000000      1.000000      0.000000     -4.735916\n",
       "25%      973.000000      2.000000      0.000000      0.472078\n",
       "50%     1738.000000      4.000000      0.000000      2.699739\n",
       "75%     3289.250000      6.000000      1.000000      4.372577\n",
       "max    98112.000000    204.000000      1.000000     12.986027"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of the entire dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6a824dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D0             float64\n",
       "D24            float64\n",
       "D48            float64\n",
       "R3             float64\n",
       "R24            float64\n",
       "gene_length      int64\n",
       "ExonN            int64\n",
       "Paralogs        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e07a2eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D0             0\n",
       "D24            0\n",
       "D48            0\n",
       "R3             0\n",
       "R24            0\n",
       "gene_length    0\n",
       "ExonN          0\n",
       "Paralogs       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check are there NA\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eab9d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAGdCAYAAADKXt17AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA140lEQVR4nO3de3RU5b3/8c+YmyEn2c3FZJwSMJ6mCAYtDTUEaIECgUrIoZ5T0OCASgEPQogEuRzbU3TVhIsG2+aIaF3QAprWAh5bMRKUg6ZcDYwSRNAWuSYE6zAhCElI9u8Py/45BBXibHLh/Vprr9V59nfv+e5H23z6zJ49DtM0TQEAACDgrmntBgAAADoqghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2CS4tRvoSJqamnTs2DFFRkbK4XC0djsAAOASmKapU6dOyeVy6ZprArsGRdAKoGPHjikxMbG12wAAAC1w+PBhde7cOaDnJGgFUGRkpKTP/kFFRUW1cjcAAOBS1NTUKDEx0fo7HkgErQA6/3FhVFQUQQsAgHbGjtt+uBkeAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwSasGrTfffFMjR46Uy+WSw+HQSy+99IW1kydPlsPh0JNPPuk3XldXp2nTpikuLk4RERHKysrSkSNH/Gq8Xq/cbrcMw5BhGHK73Tp58qRfzaFDhzRy5EhFREQoLi5OOTk5qq+vD9CVAgCAq1GrBq3Tp0/r1ltvVVFR0ZfWvfTSS9q2bZtcLlezfbm5uVq7dq2Ki4tVVlam2tpaZWZmqrGx0arJzs6Wx+NRSUmJSkpK5PF45Ha7rf2NjY0aMWKETp8+rbKyMhUXF2v16tXKy8sL3MUCAICrTnBrvvmPfvQj/ehHP/rSmqNHj2rq1Kl67bXXNGLECL99Pp9Pzz33nFasWKEhQ4ZIklauXKnExERt2LBBw4YN0969e1VSUqKtW7cqLS1NkvTss88qPT1d+/btU7du3bR+/Xq99957Onz4sBXmnnjiCd1zzz167LHHFBUVZcPVX54b5rzS2i1cto/mj/jqIgAAOrA2fY9WU1OT3G63HnroId18883N9peXl6uhoUEZGRnWmMvlUkpKijZv3ixJ2rJliwzDsEKWJPXp00eGYfjVpKSk+K2YDRs2THV1dSovL//C/urq6lRTU+O3AQAAnNemg9aCBQsUHBysnJyci+6vqqpSaGiooqOj/cYTEhJUVVVl1cTHxzc7Nj4+3q8mISHBb390dLRCQ0OtmospKCiw7vsyDEOJiYmXdX0AAKBja7NBq7y8XL/61a+0fPlyORyOyzrWNE2/Yy52fEtqLjR37lz5fD5rO3z48GX1CQAAOrY2G7TeeustVVdXq0uXLgoODlZwcLAOHjyovLw83XDDDZIkp9Op+vp6eb1ev2Orq6utFSqn06njx483O/+JEyf8ai5cufJ6vWpoaGi20vV5YWFhioqK8tsAAADOa7NBy+12691335XH47E2l8ulhx56SK+99pokKTU1VSEhISotLbWOq6ysVEVFhfr27StJSk9Pl8/n0/bt262abdu2yefz+dVUVFSosrLSqlm/fr3CwsKUmpp6JS4XAAB0QK36rcPa2lp9+OGH1usDBw7I4/EoJiZGXbp0UWxsrF99SEiInE6nunXrJkkyDEMTJkxQXl6eYmNjFRMTo5kzZ6pnz57WtxC7d++u4cOHa+LEiVq6dKkkadKkScrMzLTOk5GRoR49esjtdmvRokX65JNPNHPmTE2cOJFVKgAA0GKtuqL19ttvq1evXurVq5ckacaMGerVq5f++7//+5LPsXjxYo0aNUqjR49Wv3791KlTJ/35z39WUFCQVbNq1Sr17NlTGRkZysjI0C233KIVK1ZY+4OCgvTKK6/o2muvVb9+/TR69GiNGjVKjz/+eOAuFgAAXHUcpmmard1ER1FTUyPDMOTz+QK+EsZztAAAsIedf7/b7D1aAAAA7R1BCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJu0atB68803NXLkSLlcLjkcDr300kvWvoaGBs2ePVs9e/ZURESEXC6Xxo0bp2PHjvmdo66uTtOmTVNcXJwiIiKUlZWlI0eO+NV4vV653W4ZhiHDMOR2u3Xy5Em/mkOHDmnkyJGKiIhQXFyccnJyVF9fb9elAwCAq0CrBq3Tp0/r1ltvVVFRUbN9n376qXbu3Kmf//zn2rlzp9asWaP9+/crKyvLry43N1dr165VcXGxysrKVFtbq8zMTDU2Nlo12dnZ8ng8KikpUUlJiTwej9xut7W/sbFRI0aM0OnTp1VWVqbi4mKtXr1aeXl59l08AADo8BymaZqt3YQkORwOrV27VqNGjfrCmh07dui2227TwYMH1aVLF/l8Pl133XVasWKFxowZI0k6duyYEhMTtW7dOg0bNkx79+5Vjx49tHXrVqWlpUmStm7dqvT0dL3//vvq1q2bXn31VWVmZurw4cNyuVySpOLiYt1zzz2qrq5WVFTUJV1DTU2NDMOQz+e75GMu1Q1zXgno+a6Ej+aPaO0WAAD4Snb+/W5X92j5fD45HA594xvfkCSVl5eroaFBGRkZVo3L5VJKSoo2b94sSdqyZYsMw7BCliT16dNHhmH41aSkpFghS5KGDRumuro6lZeXf2E/dXV1qqmp8dsAAADOazdB6+zZs5ozZ46ys7OttFlVVaXQ0FBFR0f71SYkJKiqqsqqiY+Pb3a++Ph4v5qEhAS//dHR0QoNDbVqLqagoMC678swDCUmJn6tawQAAB1LuwhaDQ0NuvPOO9XU1KSnnnrqK+tN05TD4bBef/4/f52aC82dO1c+n8/aDh8+/JW9AQCAq0ebD1oNDQ0aPXq0Dhw4oNLSUr/PTp1Op+rr6+X1ev2Oqa6utlaonE6njh8/3uy8J06c8Ku5cOXK6/WqoaGh2UrX54WFhSkqKspvAwAAOK9NB63zIeuDDz7Qhg0bFBsb67c/NTVVISEhKi0ttcYqKytVUVGhvn37SpLS09Pl8/m0fft2q2bbtm3y+Xx+NRUVFaqsrLRq1q9fr7CwMKWmptp5iQAAoAMLbs03r62t1Ycffmi9PnDggDwej2JiYuRyufQf//Ef2rlzp/7yl7+osbHRWnWKiYlRaGioDMPQhAkTlJeXp9jYWMXExGjmzJnq2bOnhgwZIknq3r27hg8frokTJ2rp0qWSpEmTJikzM1PdunWTJGVkZKhHjx5yu91atGiRPvnkE82cOVMTJ05klQoAALRYqwatt99+W4MGDbJez5gxQ5I0fvx4zZs3Ty+//LIk6Tvf+Y7fcRs3btTAgQMlSYsXL1ZwcLBGjx6tM2fOaPDgwVq+fLmCgoKs+lWrViknJ8f6dmJWVpbfs7uCgoL0yiuvaMqUKerXr5/Cw8OVnZ2txx9/3I7LBgAAV4k28xytjoDnaPnjOVoAgPaA52gBAAC0QwQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbNKqQevNN9/UyJEj5XK55HA49NJLL/ntN01T8+bNk8vlUnh4uAYOHKg9e/b41dTV1WnatGmKi4tTRESEsrKydOTIEb8ar9crt9stwzBkGIbcbrdOnjzpV3Po0CGNHDlSERERiouLU05Ojurr6+24bAAAcJVo1aB1+vRp3XrrrSoqKrro/oULF6qwsFBFRUXasWOHnE6nhg4dqlOnTlk1ubm5Wrt2rYqLi1VWVqba2lplZmaqsbHRqsnOzpbH41FJSYlKSkrk8Xjkdrut/Y2NjRoxYoROnz6tsrIyFRcXa/Xq1crLy7Pv4gEAQIfnME3TbO0mJMnhcGjt2rUaNWqUpM9Ws1wul3JzczV79mxJn61eJSQkaMGCBZo8ebJ8Pp+uu+46rVixQmPGjJEkHTt2TImJiVq3bp2GDRumvXv3qkePHtq6davS0tIkSVu3blV6erref/99devWTa+++qoyMzN1+PBhuVwuSVJxcbHuueceVVdXKyoq6pKuoaamRoZhyOfzXfIxl+qGOa8E9HxXwkfzR7R2CwAAfCU7/3632Xu0Dhw4oKqqKmVkZFhjYWFhGjBggDZv3ixJKi8vV0NDg1+Ny+VSSkqKVbNlyxYZhmGFLEnq06ePDMPwq0lJSbFCliQNGzZMdXV1Ki8v/8Ie6+rqVFNT47cBAACc12aDVlVVlSQpISHBbzwhIcHaV1VVpdDQUEVHR39pTXx8fLPzx8fH+9Vc+D7R0dEKDQ21ai6moKDAuu/LMAwlJiZe5lUCAICOrM0GrfMcDoffa9M0m41d6MKai9W3pOZCc+fOlc/ns7bDhw9/aV8AAODq0maDltPplKRmK0rV1dXW6pPT6VR9fb28Xu+X1hw/frzZ+U+cOOFXc+H7eL1eNTQ0NFvp+rywsDBFRUX5bQAAAOe12aCVlJQkp9Op0tJSa6y+vl6bNm1S3759JUmpqakKCQnxq6msrFRFRYVVk56eLp/Pp+3bt1s127Ztk8/n86upqKhQZWWlVbN+/XqFhYUpNTXV1usEAAAdV3Brvnltba0+/PBD6/WBAwfk8XgUExOjLl26KDc3V/n5+UpOTlZycrLy8/PVqVMnZWdnS5IMw9CECROUl5en2NhYxcTEaObMmerZs6eGDBkiSerevbuGDx+uiRMnaunSpZKkSZMmKTMzU926dZMkZWRkqEePHnK73Vq0aJE++eQTzZw5UxMnTmSVCgAAtFirBq23335bgwYNsl7PmDFDkjR+/HgtX75cs2bN0pkzZzRlyhR5vV6lpaVp/fr1ioyMtI5ZvHixgoODNXr0aJ05c0aDBw/W8uXLFRQUZNWsWrVKOTk51rcTs7Ky/J7dFRQUpFdeeUVTpkxRv379FB4eruzsbD3++ON2TwEAAOjA2sxztDoCnqPlj+doAQDag6vyOVoAAADtHUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJi0KWgcOHAh0HwAAAB1Oi4LWt771LQ0aNEgrV67U2bNnA90TAABAh9CioPXOO++oV69eysvLk9Pp1OTJk7V9+/ZA9wYAANCutShopaSkqLCwUEePHtWyZctUVVWl/v376+abb1ZhYaFOnDgR6D4BAADana91M3xwcLB+/OMf649//KMWLFigv/3tb5o5c6Y6d+6scePGqbKyMlB9AgAAtDtfK2i9/fbbmjJliq6//noVFhZq5syZ+tvf/qY33nhDR48e1b/9278Fqk8AAIB2J7glBxUWFmrZsmXat2+fbr/9dv3+97/X7bffrmuu+Sy3JSUlaenSpbrpppsC2iwAAEB70qKgtWTJEt13332699575XQ6L1rTpUsXPffcc1+rOQAAgPasRUHrgw8++Mqa0NBQjR8/viWnBwAA6BBadI/WsmXL9OKLLzYbf/HFF/W73/3uazcFAADQEbQoaM2fP19xcXHNxuPj45Wfn/+1mwIAAOgIWhS0Dh48qKSkpGbjXbt21aFDh752UwAAAB1Bi4JWfHy83n333Wbj77zzjmJjY792UwAAAB1Bi4LWnXfeqZycHG3cuFGNjY1qbGzUG2+8oenTp+vOO+8MWHPnzp3Tz372MyUlJSk8PFw33nijHn30UTU1NVk1pmlq3rx5crlcCg8P18CBA7Vnzx6/89TV1WnatGmKi4tTRESEsrKydOTIEb8ar9crt9stwzBkGIbcbrdOnjwZsGsBAABXnxYFrV/+8pdKS0vT4MGDFR4ervDwcGVkZOiHP/xhQO/RWrBggZ5++mkVFRVp7969WrhwoRYtWqTf/OY3Vs3ChQtVWFiooqIi7dixQ06nU0OHDtWpU6esmtzcXK1du1bFxcUqKytTbW2tMjMz1djYaNVkZ2fL4/GopKREJSUl8ng8crvdAbsWAABw9XGYpmm29OD9+/frnXfeUXh4uHr27KmuXbsGsjdlZmYqISHB73lc//7v/65OnTppxYoVMk1TLpdLubm5mj17tqTPVq8SEhK0YMECTZ48WT6fT9ddd51WrFihMWPGSJKOHTumxMRErVu3TsOGDdPevXvVo0cPbd26VWlpaZKkrVu3Kj09Xe+//766det2Sf3W1NTIMAz5fD5FRUUFdC5umPNKQM93JXw0f0RrtwAAwFey8+/31/oJnm9/+9v6yU9+oszMzICHLEnq37+/Xn/9de3fv1/SZ/eAlZWV6fbbb5ckHThwQFVVVcrIyLCOCQsL04ABA7R582ZJUnl5uRoaGvxqXC6XUlJSrJotW7bIMAwrZElSnz59ZBiGVXMxdXV1qqmp8dsAAADOa9EDSxsbG7V8+XK9/vrrqq6u9rtnSpLeeOONgDQ3e/Zs+Xw+3XTTTQoKClJjY6Mee+wx3XXXXZKkqqoqSVJCQoLfcQkJCTp48KBVExoaqujo6GY154+vqqpSfHx8s/ePj4+3ai6moKBAjzzySMsvEAAAdGgtClrTp0/X8uXLNWLECKWkpMjhcAS6L0nSH/7wB61cuVLPP/+8br75Znk8HuXm5srlcvk9df7C9zdN8yt7urDmYvVfdZ65c+dqxowZ1uuamholJiZ+5XUBAICrQ4uCVnFxsf74xz9aH+HZ5aGHHtKcOXOsbzL27NlTBw8eVEFBgcaPH2/9zmJVVZWuv/5667jq6mprlcvpdKq+vl5er9dvVau6ulp9+/a1ao4fP97s/U+cONFstezzwsLCFBYW9vUvFAAAdEgtukcrNDRU3/rWtwLdSzOffvqprrnGv8WgoCDro8qkpCQ5nU6VlpZa++vr67Vp0yYrRKWmpiokJMSvprKyUhUVFVZNenq6fD6ftm/fbtVs27ZNPp/PqgEAALhcLVrRysvL069+9SsVFRXZ9rGhJI0cOVKPPfaYunTpoptvvlm7du1SYWGh7rvvPkmffdyXm5ur/Px8JScnKzk5Wfn5+erUqZOys7MlSYZhaMKECcrLy1NsbKxiYmI0c+ZM9ezZU0OGDJEkde/eXcOHD9fEiRO1dOlSSdKkSZOUmZl5yd84BAAAuFCLglZZWZk2btyoV199VTfffLNCQkL89q9ZsyYgzf3mN7/Rz3/+c02ZMkXV1dVyuVyaPHmy/vu//9uqmTVrls6cOaMpU6bI6/UqLS1N69evV2RkpFWzePFiBQcHa/To0Tpz5owGDx6s5cuXKygoyKpZtWqVcnJyrG8nZmVlqaioKCDXAQAArk4teo7Wvffe+6X7ly1b1uKG2jOeo+WP52gBANoDO/9+t2hF62oNUgAAAJejxQ8sPXfunDZs2KClS5daP3dz7Ngx1dbWBqw5AACA9qxFK1oHDx7U8OHDdejQIdXV1Wno0KGKjIzUwoULdfbsWT399NOB7hMAAKDdadGK1vTp09W7d295vV6Fh4db4z/+8Y/1+uuvB6w5AACA9qzF3zr861//qtDQUL/xrl276ujRowFpDAAAoL1r0YpWU1OTGhsbm40fOXLE77EKAAAAV7MWBa2hQ4fqySeftF47HA7V1tbqF7/4he0/ywMAANBetOijw8WLF2vQoEHq0aOHzp49q+zsbH3wwQeKi4vTCy+8EOgeAQAA2qUWBS2XyyWPx6MXXnhBO3fuVFNTkyZMmKCxY8f63RwPAABwNWtR0JKk8PBw3XfffdbvDgIAAMBfi4LW73//+y/dP27cuBY1AwAA0JG0KGhNnz7d73VDQ4M+/fRThYaGqlOnTgQtAAAAtfBbh16v12+rra3Vvn371L9/f26GBwAA+KcW/9bhhZKTkzV//vxmq10AAABXq4AFLUkKCgrSsWPHAnlKAACAdqtF92i9/PLLfq9N01RlZaWKiorUr1+/gDQGAADQ3rUoaI0aNcrvtcPh0HXXXacf/vCHeuKJJwLRFwAAQLvXoqDV1NQU6D4AAAA6nIDeowUAAID/r0UrWjNmzLjk2sLCwpa8BQAAQLvXoqC1a9cu7dy5U+fOnVO3bt0kSfv371dQUJC++93vWnUOhyMwXQIAALRDLQpaI0eOVGRkpH73u98pOjpa0mcPMb333nv1/e9/X3l5eQFtEgAAoD1q0T1aTzzxhAoKCqyQJUnR0dH65S9/ybcOAQAA/qlFQaumpkbHjx9vNl5dXa1Tp0597aYAAAA6ghYFrR//+Me699579ac//UlHjhzRkSNH9Kc//UkTJkzQHXfcEegeAQAA2qUW3aP19NNPa+bMmbr77rvV0NDw2YmCgzVhwgQtWrQooA0CAAC0Vy0KWp06ddJTTz2lRYsW6W9/+5tM09S3vvUtRUREBLo/AACAdutrPbC0srJSlZWV+va3v62IiAiZphmovgAAANq9FgWtf/zjHxo8eLC+/e1v6/bbb1dlZaUk6ac//SmPdgAAAPinFgWtBx98UCEhITp06JA6depkjY8ZM0YlJSUBaw4AAKA9a9E9WuvXr9drr72mzp07+40nJyfr4MGDAWkMAACgvWvRitbp06f9VrLO+/jjjxUWFva1mwIAAOgIWhS0fvCDH+j3v/+99drhcKipqUmLFi3SoEGDAtYcAABAe9aijw4XLVqkgQMH6u2331Z9fb1mzZqlPXv26JNPPtFf//rXQPcIAADQLrVoRatHjx569913ddttt2no0KE6ffq07rjjDu3atUv/+q//GugeAQAA2qXLXtFqaGhQRkaGli5dqkceecSOngAAADqEy17RCgkJUUVFhRwOhx39AAAAdBgt+uhw3Lhxeu655wLdCwAAQIfSopvh6+vr9dvf/lalpaXq3bt3s984LCwsDEhzAAAA7dllrWj9/e9/V1NTkyoqKvTd735XUVFR2r9/v3bt2mVtHo8noA0ePXpUd999t2JjY9WpUyd95zvfUXl5ubXfNE3NmzdPLpdL4eHhGjhwoPbs2eN3jrq6Ok2bNk1xcXGKiIhQVlaWjhw54lfj9XrldrtlGIYMw5Db7dbJkycDei0AAODqcllBKzk5WR9//LE2btyojRs3Kj4+XsXFxdbrjRs36o033ghYc16vV/369VNISIheffVVvffee3riiSf0jW98w6pZuHChCgsLVVRUpB07dsjpdGro0KE6deqUVZObm6u1a9equLhYZWVlqq2tVWZmphobG62a7OxseTwelZSUqKSkRB6PR263O2DXAgAArj4O0zTNSy2+5pprVFVVpfj4eElSVFSUPB6PbrzxRluamzNnjv7617/qrbfeuuh+0zTlcrmUm5ur2bNnS/ps9SohIUELFizQ5MmT5fP5dN1112nFihUaM2aMJOnYsWNKTEzUunXrNGzYMO3du1c9evTQ1q1blZaWJknaunWr0tPT9f7776tbt26X1G9NTY0Mw5DP51NUVFQAZuD/u2HOKwE935Xw0fwRrd0CAABfyc6/3y26Gf68y8hoLfLyyy+rd+/e+slPfqL4+Hj16tVLzz77rLX/wIEDqqqqUkZGhjUWFhamAQMGaPPmzZKk8vJy65EU57lcLqWkpFg1W7ZskWEYVsiSpD59+sgwDKvmYurq6lRTU+O3AQAAnHdZQcvhcDR7rIOdj3n4+9//riVLlig5OVmvvfaa7r//fuXk5Fg//1NVVSVJSkhI8DsuISHB2ldVVaXQ0FBFR0d/ac35VbrPi4+Pt2oupqCgwLqnyzAMJSYmtvxiAQBAh3NZ3zo0TVP33HOP9cPRZ8+e1f3339/sW4dr1qwJSHNNTU3q3bu38vPzJUm9evXSnj17tGTJEo0bN86quzDsmab5lQHwwpqL1X/VeebOnasZM2ZYr2tqaghbAADAcllBa/z48X6v77777oA2c6Hrr79ePXr08Bvr3r27Vq9eLUlyOp2SPluRuv76662a6upqa5XL6XSqvr5eXq/Xb1Wrurpaffv2tWqOHz/e7P1PnDjRbLXs88LCwqzQCQAAcKHLClrLli2zq4+L6tevn/bt2+c3tn//fnXt2lWSlJSUJKfTqdLSUvXq1UvSZ8/42rRpkxYsWCBJSk1NVUhIiEpLSzV69GhJUmVlpSoqKrRw4UJJUnp6unw+n7Zv367bbrtNkrRt2zb5fD4rjAEAAFyuFj2w9Ep58MEH1bdvX+Xn52v06NHavn27nnnmGT3zzDOSPvu4Lzc3V/n5+UpOTlZycrLy8/PVqVMnZWdnS5IMw9CECROUl5en2NhYxcTEaObMmerZs6eGDBki6bNVsuHDh2vixIlaunSpJGnSpEnKzMy85G8cAgAAXKhNB63vfe97Wrt2rebOnatHH31USUlJevLJJzV27FirZtasWTpz5oymTJkir9ertLQ0rV+/XpGRkVbN4sWLFRwcrNGjR+vMmTMaPHiwli9frqCgIKtm1apVysnJsb6dmJWVpaKioit3sQAAoMO5rOdo4cvxHC1/PEcLANAetNnnaAEAAOCLEbQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwSbsKWgUFBXI4HMrNzbXGTNPUvHnz5HK5FB4eroEDB2rPnj1+x9XV1WnatGmKi4tTRESEsrKydOTIEb8ar9crt9stwzBkGIbcbrdOnjx5Ba4KAAB0VO0maO3YsUPPPPOMbrnlFr/xhQsXqrCwUEVFRdqxY4ecTqeGDh2qU6dOWTW5ublau3atiouLVVZWptraWmVmZqqxsdGqyc7OlsfjUUlJiUpKSuTxeOR2u6/Y9QEAgI6nXQSt2tpajR07Vs8++6yio6OtcdM09eSTT+rhhx/WHXfcoZSUFP3ud7/Tp59+queff16S5PP59Nxzz+mJJ57QkCFD1KtXL61cuVK7d+/Whg0bJEl79+5VSUmJfvvb3yo9PV3p6el69tln9Ze//EX79u1rlWsGAADtX7sIWg888IBGjBihIUOG+I0fOHBAVVVVysjIsMbCwsI0YMAAbd68WZJUXl6uhoYGvxqXy6WUlBSrZsuWLTIMQ2lpaVZNnz59ZBiGVXMxdXV1qqmp8dsAAADOC27tBr5KcXGxdu7cqR07djTbV1VVJUlKSEjwG09ISNDBgwetmtDQUL+VsPM154+vqqpSfHx8s/PHx8dbNRdTUFCgRx555PIuCAAAXDXa9IrW4cOHNX36dK1cuVLXXnvtF9Y5HA6/16ZpNhu70IU1F6v/qvPMnTtXPp/P2g4fPvyl7wkAAK4ubTpolZeXq7q6WqmpqQoODlZwcLA2bdqkX//61woODrZWsi5cdaqurrb2OZ1O1dfXy+v1fmnN8ePHm73/iRMnmq2WfV5YWJiioqL8NgAAgPPadNAaPHiwdu/eLY/HY229e/fW2LFj5fF4dOONN8rpdKq0tNQ6pr6+Xps2bVLfvn0lSampqQoJCfGrqaysVEVFhVWTnp4un8+n7du3WzXbtm2Tz+ezagAAAC5Xm75HKzIyUikpKX5jERERio2NtcZzc3OVn5+v5ORkJScnKz8/X506dVJ2drYkyTAMTZgwQXl5eYqNjVVMTIxmzpypnj17WjfXd+/eXcOHD9fEiRO1dOlSSdKkSZOUmZmpbt26XcErBgAAHUmbDlqXYtasWTpz5oymTJkir9ertLQ0rV+/XpGRkVbN4sWLFRwcrNGjR+vMmTMaPHiwli9frqCgIKtm1apVysnJsb6dmJWVpaKioit+PQAAoONwmKZptnYTHUVNTY0Mw5DP5wv4/Vo3zHkloOe7Ej6aP6K1WwAA4CvZ+fe7Td+jBQAA0J4RtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALBJmw5aBQUF+t73vqfIyEjFx8dr1KhR2rdvn1+NaZqaN2+eXC6XwsPDNXDgQO3Zs8evpq6uTtOmTVNcXJwiIiKUlZWlI0eO+NV4vV653W4ZhiHDMOR2u3Xy5Em7LxEAAHRgbTpobdq0SQ888IC2bt2q0tJSnTt3ThkZGTp9+rRVs3DhQhUWFqqoqEg7duyQ0+nU0KFDderUKasmNzdXa9euVXFxscrKylRbW6vMzEw1NjZaNdnZ2fJ4PCopKVFJSYk8Ho/cbvcVvV4AANCxOEzTNFu7iUt14sQJxcfHa9OmTfrBD34g0zTlcrmUm5ur2bNnS/ps9SohIUELFizQ5MmT5fP5dN1112nFihUaM2aMJOnYsWNKTEzUunXrNGzYMO3du1c9evTQ1q1blZaWJknaunWr0tPT9f7776tbt26X1F9NTY0Mw5DP51NUVFRAr/2GOa8E9HxXwkfzR7R2CwAAfCU7/3636RWtC/l8PklSTEyMJOnAgQOqqqpSRkaGVRMWFqYBAwZo8+bNkqTy8nI1NDT41bhcLqWkpFg1W7ZskWEYVsiSpD59+sgwDKvmYurq6lRTU+O3AQAAnNdugpZpmpoxY4b69++vlJQUSVJVVZUkKSEhwa82ISHB2ldVVaXQ0FBFR0d/aU18fHyz94yPj7dqLqagoMC6p8swDCUmJrb8AgEAQIfTboLW1KlT9e677+qFF15ots/hcPi9Nk2z2diFLqy5WP1XnWfu3Lny+XzWdvjw4a+6DAAAcBVpF0Fr2rRpevnll7Vx40Z17tzZGnc6nZLUbNWpurraWuVyOp2qr6+X1+v90prjx483e98TJ040Wy37vLCwMEVFRfltAAAA57XpoGWapqZOnao1a9bojTfeUFJSkt/+pKQkOZ1OlZaWWmP19fXatGmT+vbtK0lKTU1VSEiIX01lZaUqKiqsmvT0dPl8Pm3fvt2q2bZtm3w+n1UDAABwuYJbu4Ev88ADD+j555/X//7v/yoyMtJauTIMQ+Hh4XI4HMrNzVV+fr6Sk5OVnJys/Px8derUSdnZ2VbthAkTlJeXp9jYWMXExGjmzJnq2bOnhgwZIknq3r27hg8frokTJ2rp0qWSpEmTJikzM/OSv3EIAABwoTYdtJYsWSJJGjhwoN/4smXLdM8990iSZs2apTNnzmjKlCnyer1KS0vT+vXrFRkZadUvXrxYwcHBGj16tM6cOaPBgwdr+fLlCgoKsmpWrVqlnJwc69uJWVlZKioqsvcCAQBAh9aunqPV1vEcLX88RwsA0B7wHC0AAIB2iKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANiFoAQAA2ISgBQAAYBOCFgAAgE0IWgAAADYhaAEAANiEoAUAAGATghYAAIBNCFoAAAA2IWgBAADYhKAFAABgE4IWAACATQhaAAAANglu7QbQcd0w55XWbuGyfTR/RGu3AADoQFjRAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAmBC0AAACbELQAAABsQtACAACwCU+GBz6Hp9kDAAKJFa0LPPXUU0pKStK1116r1NRUvfXWW63dEgAAaKcIWp/zhz/8Qbm5uXr44Ye1a9cuff/739ePfvQjHTp0qLVbAwAA7RBB63MKCws1YcIE/fSnP1X37t315JNPKjExUUuWLGnt1gAAQDvEPVr/VF9fr/Lycs2ZM8dvPCMjQ5s3b77oMXV1daqrq7Ne+3w+SVJNTU3A+2uq+zTg50TH0OXBF1u7hatGxSPDWrsFADY4/3fbNM2An5ug9U8ff/yxGhsblZCQ4DeekJCgqqqqix5TUFCgRx55pNl4YmKiLT0CaF3Gk63dAQA7nTp1SoZhBPScBK0LOBwOv9emaTYbO2/u3LmaMWOG9bqpqUmffPKJYmNjv/CYy1VTU6PExEQdPnxYUVFRATknvhxzfmUx31cW831lMd9XVkvn2zRNnTp1Si6XK+A9EbT+KS4uTkFBQc1Wr6qrq5utcp0XFhamsLAwv7FvfOMbtvQXFRXFf0mvMOb8ymK+ryzm+8pivq+slsx3oFeyzuNm+H8KDQ1VamqqSktL/cZLS0vVt2/fVuoKAAC0Z6xofc6MGTPkdrvVu3dvpaen65lnntGhQ4d0//33t3ZrAACgHSJofc6YMWP0j3/8Q48++qgqKyuVkpKidevWqWvXrq3WU1hYmH7xi180+4gS9mHOryzm+8pivq8s5vvKaovz7TDt+C4jAAAAuEcLAADALgQtAAAAmxC0AAAAbELQAgAAsAlBq4176qmnlJSUpGuvvVapqal66623WrulNqWgoEDf+973FBkZqfj4eI0aNUr79u3zqzFNU/PmzZPL5VJ4eLgGDhyoPXv2+NXU1dVp2rRpiouLU0REhLKysnTkyBG/Gq/XK7fbLcMwZBiG3G63Tp486Vdz6NAhjRw5UhEREYqLi1NOTo7q6+ttufa2oKCgQA6HQ7m5udYY8x1YR48e1d13363Y2Fh16tRJ3/nOd1ReXm7tZ74D69y5c/rZz36mpKQkhYeH68Ybb9Sjjz6qpqYmq4Y5b7k333xTI0eOlMvlksPh0EsvveS3v63N7e7duzVgwACFh4frm9/8ph599NHL/z1EE21WcXGxGRISYj777LPme++9Z06fPt2MiIgwDx482NqttRnDhg0zly1bZlZUVJgej8ccMWKE2aVLF7O2ttaqmT9/vhkZGWmuXr3a3L17tzlmzBjz+uuvN2tqaqya+++/3/zmN79plpaWmjt37jQHDRpk3nrrrea5c+esmuHDh5spKSnm5s2bzc2bN5spKSlmZmamtf/cuXNmSkqKOWjQIHPnzp1maWmp6XK5zKlTp16ZybjCtm/fbt5www3mLbfcYk6fPt0aZ74D55NPPjG7du1q3nPPPea2bdvMAwcOmBs2bDA//PBDq4b5Dqxf/vKXZmxsrPmXv/zFPHDggPniiy+a//Iv/2I++eSTVg1z3nLr1q0zH374YXP16tWmJHPt2rV++9vS3Pp8PjMhIcG88847zd27d5urV682IyMjzccff/yyrpmg1Ybddttt5v333+83dtNNN5lz5sxppY7avurqalOSuWnTJtM0TbOpqcl0Op3m/PnzrZqzZ8+ahmGYTz/9tGmapnny5EkzJCTELC4utmqOHj1qXnPNNWZJSYlpmqb53nvvmZLMrVu3WjVbtmwxJZnvv/++aZqf/Q/INddcYx49etSqeeGFF8ywsDDT5/PZd9Gt4NSpU2ZycrJZWlpqDhgwwApazHdgzZ492+zfv/8X7me+A2/EiBHmfffd5zd2xx13mHfffbdpmsx5IF0YtNra3D711FOmYRjm2bNnrZqCggLT5XKZTU1Nl3ydfHTYRtXX16u8vFwZGRl+4xkZGdq8eXMrddX2+Xw+SVJMTIwk6cCBA6qqqvKbx7CwMA0YMMCax/LycjU0NPjVuFwupaSkWDVbtmyRYRhKS0uzavr06SPDMPxqUlJS/H6UdNiwYaqrq/P7qKcjeOCBBzRixAgNGTLEb5z5DqyXX35ZvXv31k9+8hPFx8erV69eevbZZ639zHfg9e/fX6+//rr2798vSXrnnXdUVlam22+/XRJzbqe2NrdbtmzRgAED/B5+OmzYMB07dkwfffTRJV8XT4Zvoz7++GM1NjY2+0HrhISEZj98jc+YpqkZM2aof//+SklJkSRrri42jwcPHrRqQkNDFR0d3azm/PFVVVWKj49v9p7x8fF+NRe+T3R0tEJDQzvUP7Pi4mLt3LlTO3bsaLaP+Q6sv//971qyZIlmzJih//qv/9L27duVk5OjsLAwjRs3jvm2wezZs+Xz+XTTTTcpKChIjY2Neuyxx3TXXXdJ4t9xO7W1ua2qqtINN9zQ7H3O70tKSrqk6yJotXEOh8PvtWmazcbwmalTp+rdd99VWVlZs30tmccLay5W35Ka9uzw4cOaPn261q9fr2uvvfYL65jvwGhqalLv3r2Vn58vSerVq5f27NmjJUuWaNy4cVYd8x04f/jDH7Ry5Uo9//zzuvnmm+XxeJSbmyuXy6Xx48dbdcy5fdrS3F6sly869ovw0WEbFRcXp6CgoGb/r6W6urpZCoc0bdo0vfzyy9q4caM6d+5sjTudTkn60nl0Op2qr6+X1+v90prjx483e98TJ0741Vz4Pl6vVw0NDR3mn1l5ebmqq6uVmpqq4OBgBQcHa9OmTfr1r3+t4OBgv/+393nMd8tcf/316tGjh99Y9+7ddejQIUn8+22Hhx56SHPmzNGdd96pnj17yu1268EHH1RBQYEk5txObW1uL1ZTXV0tqfmq25chaLVRoaGhSk1NVWlpqd94aWmp+vbt20pdtT2maWrq1Klas2aN3njjjWZLuUlJSXI6nX7zWF9fr02bNlnzmJqaqpCQEL+ayspKVVRUWDXp6eny+Xzavn27VbNt2zb5fD6/moqKClVWVlo169evV1hYmFJTUwN/8a1g8ODB2r17tzwej7X17t1bY8eOlcfj0Y033sh8B1C/fv2aPa5k//791g/d8+934H366ae65hr/P41BQUHW4x2Yc/u0tblNT0/Xm2++6ffIh/Xr18vlcjX7SPFLXfJt87jizj/e4bnnnjPfe+89Mzc314yIiDA/+uij1m6tzfjP//xP0zAM8//+7//MyspKa/v000+tmvnz55uGYZhr1qwxd+/ebd51110X/bpw586dzQ0bNpg7d+40f/jDH17068K33HKLuWXLFnPLli1mz549L/p14cGDB5s7d+40N2zYYHbu3LldfxX7Unz+W4emyXwH0vbt283g4GDzscceMz/44ANz1apVZqdOncyVK1daNcx3YI0fP9785je/aT3eYc2aNWZcXJw5a9Ysq4Y5b7lTp06Zu3btMnft2mVKMgsLC81du3ZZjy1qS3N78uRJMyEhwbzrrrvM3bt3m2vWrDGjoqJ4vENH8z//8z9m165dzdDQUPO73/2u9dgCfEbSRbdly5ZZNU1NTeYvfvEL0+l0mmFhYeYPfvADc/fu3X7nOXPmjDl16lQzJibGDA8PNzMzM81Dhw751fzjH/8wx44da0ZGRpqRkZHm2LFjTa/X61dz8OBBc8SIEWZ4eLgZExNjTp061e+rwR3RhUGL+Q6sP//5z2ZKSooZFhZm3nTTTeYzzzzjt5/5Dqyamhpz+vTpZpcuXcxrr73WvPHGG82HH37YrKurs2qY85bbuHHjRf83e/z48aZptr25fffdd83vf//7ZlhYmOl0Os158+Zd1qMdTNM0HaZ5uY84BQAAwKXgHi0AAACbELQAAABsQtACAACwCUELAADAJgQtAAAAmxC0AAAAbELQAgAAsAlBCwAAwCYELQAAAJsQtAAAAGxC0AIAALAJQQsAAMAm/w+KGcJHV3ssXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create histogram plot of gene_length\n",
    "df['gene_length'].plot(kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c798b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Paralogs'] = df['Paralogs'].replace({'P':1, 'O':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a339129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D24 - D0',\n",
       " 'D48 - D0',\n",
       " 'D48 - D24',\n",
       " 'R24 - D0',\n",
       " 'R24 - D24',\n",
       " 'R24 - D48',\n",
       " 'R24 - R3',\n",
       " 'R3 - D0',\n",
       " 'R3 - D24',\n",
       " 'R3 - D48'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Find highly correlated pairs of variables\n",
    "highly_corr = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = corr_matrix.columns[i] + ' - ' + corr_matrix.columns[j]\n",
    "            highly_corr.add(colname)\n",
    "            \n",
    "highly_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a0b01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As most of expression columns are correlated with each other and their distributions are very similar\n",
    "# it makes sense to make one average column\n",
    "df['expression'] = df[['D0', 'D24', 'D48', 'R3', 'R24']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c61fe893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop used columns\n",
    "L_of_drops = ['D0', 'D24', 'D48', 'R3', 'R24']\n",
    "for i in range(5):\n",
    "    df = df.drop(L_of_drops[i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b915c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into input (X) and output (y) variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[[\"ExonN\", \"expression\", \"Paralogs\"]]\n",
    "y = df['gene_length']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[[\"ExonN\", \"expression\", \"Paralogs\"]], df[\"gene_length\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7496635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start from a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "459c9053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('R-squared:', 0.2388212085251593)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The score is very low, so should try other algorithms\n",
    "score = model.score(X_test, y_test)\n",
    "\"R-squared:\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f80cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Best max_depth:', 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try Random Forest Regressor. Should also use GridSearch to find the best max_depth\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_depth': [i for i in range(2, 21)]}\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid=params, cv=5, scoring='r2')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "'Best max_depth:', grid_search.best_params_['max_depth']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "095cac63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=4, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=4, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=4, random_state=42)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Random Forest Regressor with max_depth 4\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d9de461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the testing dataset\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00a22aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49f4775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-squared:0.3081759984338581'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'R-squared:{r2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c8a82cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MSE:25793892.47742226'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'MSE:{mse}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor doesn't perform well, as we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4bf844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Neural  Networks. Feed forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc6de65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First off, the features must be scaled and  there must be a validation dataset, therefore:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "478851be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 41998516.0000 - mse: 41998516.0000 - mae: 3369.9482 - val_loss: 35420292.0000 - val_mse: 35420288.0000 - val_mae: 3158.4077\n",
      "Epoch 2/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 41802636.0000 - mse: 41802636.0000 - mae: 3339.6086 - val_loss: 35032448.0000 - val_mse: 35032448.0000 - val_mae: 3095.3757\n",
      "Epoch 3/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 41041468.0000 - mse: 41041468.0000 - mae: 3222.8071 - val_loss: 33932492.0000 - val_mse: 33932492.0000 - val_mae: 2909.2009\n",
      "Epoch 4/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 39435892.0000 - mse: 39435892.0000 - mae: 2969.9370 - val_loss: 32049724.0000 - val_mse: 32049724.0000 - val_mae: 2595.9138\n",
      "Epoch 5/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 37111108.0000 - mse: 37111108.0000 - mae: 2662.2930 - val_loss: 29754364.0000 - val_mse: 29754364.0000 - val_mae: 2314.1235\n",
      "Epoch 6/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 34662620.0000 - mse: 34662620.0000 - mae: 2475.9946 - val_loss: 27774548.0000 - val_mse: 27774548.0000 - val_mae: 2236.9382\n",
      "Epoch 7/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 32790726.0000 - mse: 32790726.0000 - mae: 2492.8818 - val_loss: 26597412.0000 - val_mse: 26597412.0000 - val_mae: 2340.7761\n",
      "Epoch 8/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31755070.0000 - mse: 31755070.0000 - mae: 2615.6418 - val_loss: 26107824.0000 - val_mse: 26107824.0000 - val_mae: 2484.3918\n",
      "Epoch 9/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31296382.0000 - mse: 31296382.0000 - mae: 2738.4277 - val_loss: 25915266.0000 - val_mse: 25915266.0000 - val_mae: 2577.6599\n",
      "Epoch 10/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31070840.0000 - mse: 31070840.0000 - mae: 2806.8218 - val_loss: 25780474.0000 - val_mse: 25780474.0000 - val_mae: 2619.0159\n",
      "Epoch 11/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30907270.0000 - mse: 30907270.0000 - mae: 2836.5427 - val_loss: 25666678.0000 - val_mse: 25666678.0000 - val_mae: 2645.3665\n",
      "Epoch 12/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30778828.0000 - mse: 30778828.0000 - mae: 2862.9824 - val_loss: 25548088.0000 - val_mse: 25548088.0000 - val_mae: 2645.9409\n",
      "Epoch 13/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30664232.0000 - mse: 30664232.0000 - mae: 2866.7595 - val_loss: 25445298.0000 - val_mse: 25445298.0000 - val_mae: 2645.5601\n",
      "Epoch 14/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30562638.0000 - mse: 30562638.0000 - mae: 2852.0049 - val_loss: 25354000.0000 - val_mse: 25354000.0000 - val_mae: 2647.5376\n",
      "Epoch 15/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30468978.0000 - mse: 30468978.0000 - mae: 2866.3943 - val_loss: 25266400.0000 - val_mse: 25266400.0000 - val_mae: 2643.5857\n",
      "Epoch 16/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30381990.0000 - mse: 30381992.0000 - mae: 2863.2449 - val_loss: 25186006.0000 - val_mse: 25186006.0000 - val_mae: 2649.4626\n",
      "Epoch 17/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30297498.0000 - mse: 30297498.0000 - mae: 2869.2195 - val_loss: 25109856.0000 - val_mse: 25109856.0000 - val_mae: 2647.5669\n",
      "Epoch 18/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30220874.0000 - mse: 30220874.0000 - mae: 2862.1667 - val_loss: 25031668.0000 - val_mse: 25031670.0000 - val_mae: 2635.8376\n",
      "Epoch 19/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30150076.0000 - mse: 30150076.0000 - mae: 2862.5044 - val_loss: 24968482.0000 - val_mse: 24968482.0000 - val_mae: 2641.1936\n",
      "Epoch 20/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30086340.0000 - mse: 30086340.0000 - mae: 2867.6499 - val_loss: 24896706.0000 - val_mse: 24896706.0000 - val_mae: 2626.4822\n",
      "Epoch 21/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30024064.0000 - mse: 30024064.0000 - mae: 2861.4824 - val_loss: 24842788.0000 - val_mse: 24842788.0000 - val_mae: 2630.0771\n",
      "Epoch 22/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29965670.0000 - mse: 29965670.0000 - mae: 2869.6328 - val_loss: 24790428.0000 - val_mse: 24790428.0000 - val_mae: 2630.2627\n",
      "Epoch 23/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29910662.0000 - mse: 29910662.0000 - mae: 2858.7764 - val_loss: 24742318.0000 - val_mse: 24742318.0000 - val_mae: 2631.2004\n",
      "Epoch 24/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29857674.0000 - mse: 29857674.0000 - mae: 2868.4126 - val_loss: 24695004.0000 - val_mse: 24695004.0000 - val_mae: 2632.5002\n",
      "Epoch 25/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29807998.0000 - mse: 29807998.0000 - mae: 2855.0222 - val_loss: 24645812.0000 - val_mse: 24645812.0000 - val_mae: 2626.2251\n",
      "Epoch 26/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29758754.0000 - mse: 29758754.0000 - mae: 2869.7947 - val_loss: 24600194.0000 - val_mse: 24600194.0000 - val_mae: 2622.4639\n",
      "Epoch 27/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29711178.0000 - mse: 29711178.0000 - mae: 2846.7380 - val_loss: 24552860.0000 - val_mse: 24552860.0000 - val_mae: 2612.6206\n",
      "Epoch 28/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29663022.0000 - mse: 29663022.0000 - mae: 2851.8201 - val_loss: 24515972.0000 - val_mse: 24515972.0000 - val_mae: 2622.4558\n",
      "Epoch 29/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29614120.0000 - mse: 29614118.0000 - mae: 2865.6594 - val_loss: 24467196.0000 - val_mse: 24467196.0000 - val_mae: 2616.3970\n",
      "Epoch 30/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29567984.0000 - mse: 29567984.0000 - mae: 2846.9075 - val_loss: 24425320.0000 - val_mse: 24425320.0000 - val_mae: 2616.4346\n",
      "Epoch 31/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29519870.0000 - mse: 29519870.0000 - mae: 2841.3391 - val_loss: 24382886.0000 - val_mse: 24382886.0000 - val_mae: 2617.3958\n",
      "Epoch 32/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29469666.0000 - mse: 29469666.0000 - mae: 2833.8723 - val_loss: 24342182.0000 - val_mse: 24342182.0000 - val_mae: 2619.0198\n",
      "Epoch 33/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29417082.0000 - mse: 29417082.0000 - mae: 2845.9421 - val_loss: 24292654.0000 - val_mse: 24292654.0000 - val_mae: 2610.0161\n",
      "Epoch 34/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29364506.0000 - mse: 29364506.0000 - mae: 2847.5266 - val_loss: 24240938.0000 - val_mse: 24240938.0000 - val_mae: 2601.5195\n",
      "Epoch 35/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29312386.0000 - mse: 29312384.0000 - mae: 2836.4114 - val_loss: 24190990.0000 - val_mse: 24190990.0000 - val_mae: 2594.7271\n",
      "Epoch 36/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29255716.0000 - mse: 29255716.0000 - mae: 2838.3081 - val_loss: 24149216.0000 - val_mse: 24149216.0000 - val_mae: 2602.6150\n",
      "Epoch 37/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29197990.0000 - mse: 29197990.0000 - mae: 2829.5740 - val_loss: 24094276.0000 - val_mse: 24094276.0000 - val_mae: 2593.8042\n",
      "Epoch 38/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29137700.0000 - mse: 29137700.0000 - mae: 2832.4224 - val_loss: 24043876.0000 - val_mse: 24043876.0000 - val_mae: 2591.0962\n",
      "Epoch 39/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29081142.0000 - mse: 29081142.0000 - mae: 2827.0923 - val_loss: 23991786.0000 - val_mse: 23991786.0000 - val_mae: 2591.8191\n",
      "Epoch 40/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29013592.0000 - mse: 29013592.0000 - mae: 2830.7988 - val_loss: 23932516.0000 - val_mse: 23932516.0000 - val_mae: 2583.9487\n",
      "Epoch 41/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28946690.0000 - mse: 28946690.0000 - mae: 2817.0510 - val_loss: 23875244.0000 - val_mse: 23875242.0000 - val_mae: 2581.5276\n",
      "Epoch 42/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 28880424.0000 - mse: 28880424.0000 - mae: 2823.6160 - val_loss: 23826214.0000 - val_mse: 23826214.0000 - val_mae: 2593.7700\n",
      "Epoch 43/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28808388.0000 - mse: 28808388.0000 - mae: 2804.7588 - val_loss: 23764204.0000 - val_mse: 23764204.0000 - val_mae: 2587.3750\n",
      "Epoch 44/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 28734044.0000 - mse: 28734046.0000 - mae: 2812.4956 - val_loss: 23697144.0000 - val_mse: 23697144.0000 - val_mae: 2582.1201\n",
      "Epoch 45/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28656152.0000 - mse: 28656152.0000 - mae: 2821.4741 - val_loss: 23615348.0000 - val_mse: 23615348.0000 - val_mae: 2558.0015\n",
      "Epoch 46/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 28577370.0000 - mse: 28577370.0000 - mae: 2801.7305 - val_loss: 23550594.0000 - val_mse: 23550594.0000 - val_mae: 2561.8635\n",
      "Epoch 47/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28492044.0000 - mse: 28492044.0000 - mae: 2776.5105 - val_loss: 23487072.0000 - val_mse: 23487072.0000 - val_mae: 2569.9639\n",
      "Epoch 48/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28403282.0000 - mse: 28403282.0000 - mae: 2807.7947 - val_loss: 23404890.0000 - val_mse: 23404890.0000 - val_mae: 2561.0127\n",
      "Epoch 49/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28311498.0000 - mse: 28311494.0000 - mae: 2778.3262 - val_loss: 23328550.0000 - val_mse: 23328550.0000 - val_mae: 2558.3186\n",
      "Epoch 50/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28217932.0000 - mse: 28217932.0000 - mae: 2787.2266 - val_loss: 23229164.0000 - val_mse: 23229164.0000 - val_mae: 2529.5566\n",
      "Epoch 51/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28115068.0000 - mse: 28115068.0000 - mae: 2770.3440 - val_loss: 23146278.0000 - val_mse: 23146278.0000 - val_mae: 2533.7522\n",
      "Epoch 52/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28007212.0000 - mse: 28007212.0000 - mae: 2762.4465 - val_loss: 23056870.0000 - val_mse: 23056870.0000 - val_mae: 2535.0762\n",
      "Epoch 53/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27897058.0000 - mse: 27897058.0000 - mae: 2745.3889 - val_loss: 22974268.0000 - val_mse: 22974268.0000 - val_mae: 2543.7825\n",
      "Epoch 54/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27783724.0000 - mse: 27783724.0000 - mae: 2758.6331 - val_loss: 22865800.0000 - val_mse: 22865800.0000 - val_mae: 2525.6321\n",
      "Epoch 55/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27658478.0000 - mse: 27658478.0000 - mae: 2743.1416 - val_loss: 22751112.0000 - val_mse: 22751112.0000 - val_mae: 2513.5322\n",
      "Epoch 56/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27531106.0000 - mse: 27531106.0000 - mae: 2740.9041 - val_loss: 22647654.0000 - val_mse: 22647654.0000 - val_mae: 2512.4624\n",
      "Epoch 57/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27400102.0000 - mse: 27400102.0000 - mae: 2739.3914 - val_loss: 22513582.0000 - val_mse: 22513582.0000 - val_mae: 2480.9893\n",
      "Epoch 58/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27264040.0000 - mse: 27264040.0000 - mae: 2703.5659 - val_loss: 22397462.0000 - val_mse: 22397460.0000 - val_mae: 2477.4048\n",
      "Epoch 59/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27117402.0000 - mse: 27117402.0000 - mae: 2706.7615 - val_loss: 22283602.0000 - val_mse: 22283602.0000 - val_mae: 2482.8677\n",
      "Epoch 60/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26966416.0000 - mse: 26966416.0000 - mae: 2692.0830 - val_loss: 22146488.0000 - val_mse: 22146488.0000 - val_mae: 2470.2517\n",
      "Epoch 61/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26813370.0000 - mse: 26813370.0000 - mae: 2682.2043 - val_loss: 22014916.0000 - val_mse: 22014916.0000 - val_mae: 2468.0479\n",
      "Epoch 62/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26653748.0000 - mse: 26653748.0000 - mae: 2673.0327 - val_loss: 21866184.0000 - val_mse: 21866182.0000 - val_mae: 2446.8904\n",
      "Epoch 63/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26488778.0000 - mse: 26488778.0000 - mae: 2649.8435 - val_loss: 21724932.0000 - val_mse: 21724932.0000 - val_mae: 2435.7561\n",
      "Epoch 64/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26321250.0000 - mse: 26321250.0000 - mae: 2650.8699 - val_loss: 21564430.0000 - val_mse: 21564430.0000 - val_mae: 2407.8674\n",
      "Epoch 65/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26150106.0000 - mse: 26150106.0000 - mae: 2617.3618 - val_loss: 21430324.0000 - val_mse: 21430324.0000 - val_mae: 2416.0745\n",
      "Epoch 66/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25975040.0000 - mse: 25975040.0000 - mae: 2610.4619 - val_loss: 21298286.0000 - val_mse: 21298286.0000 - val_mae: 2436.0061\n",
      "Epoch 67/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 25793964.0000 - mse: 25793962.0000 - mae: 2621.7249 - val_loss: 21106022.0000 - val_mse: 21106022.0000 - val_mae: 2374.2017\n",
      "Epoch 68/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25617132.0000 - mse: 25617132.0000 - mae: 2586.2522 - val_loss: 20952898.0000 - val_mse: 20952898.0000 - val_mae: 2366.3162\n",
      "Epoch 69/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25437732.0000 - mse: 25437732.0000 - mae: 2574.6846 - val_loss: 20794160.0000 - val_mse: 20794160.0000 - val_mae: 2358.1238\n",
      "Epoch 70/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25250210.0000 - mse: 25250210.0000 - mae: 2561.7917 - val_loss: 20626356.0000 - val_mse: 20626356.0000 - val_mae: 2335.8157\n",
      "Epoch 71/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25064526.0000 - mse: 25064528.0000 - mae: 2540.4448 - val_loss: 20467714.0000 - val_mse: 20467716.0000 - val_mae: 2329.1917\n",
      "Epoch 72/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24878814.0000 - mse: 24878814.0000 - mae: 2544.7375 - val_loss: 20292996.0000 - val_mse: 20292996.0000 - val_mae: 2302.0676\n",
      "Epoch 73/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24700290.0000 - mse: 24700290.0000 - mae: 2519.9912 - val_loss: 20131572.0000 - val_mse: 20131572.0000 - val_mae: 2289.6992\n",
      "Epoch 74/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 24528942.0000 - mse: 24528942.0000 - mae: 2489.5874 - val_loss: 19970518.0000 - val_mse: 19970518.0000 - val_mae: 2262.1980\n",
      "Epoch 75/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24352130.0000 - mse: 24352132.0000 - mae: 2476.4163 - val_loss: 19821380.0000 - val_mse: 19821380.0000 - val_mae: 2256.3428\n",
      "Epoch 76/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24186798.0000 - mse: 24186798.0000 - mae: 2468.2061 - val_loss: 19678752.0000 - val_mse: 19678752.0000 - val_mae: 2250.7219\n",
      "Epoch 77/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24021324.0000 - mse: 24021324.0000 - mae: 2448.0972 - val_loss: 19536694.0000 - val_mse: 19536694.0000 - val_mae: 2242.6707\n",
      "Epoch 78/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23863532.0000 - mse: 23863532.0000 - mae: 2426.4504 - val_loss: 19411716.0000 - val_mse: 19411716.0000 - val_mae: 2257.5784\n",
      "Epoch 79/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23719762.0000 - mse: 23719762.0000 - mae: 2427.0298 - val_loss: 19274332.0000 - val_mse: 19274332.0000 - val_mae: 2250.8979\n",
      "Epoch 80/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23562376.0000 - mse: 23562376.0000 - mae: 2394.3137 - val_loss: 19148474.0000 - val_mse: 19148474.0000 - val_mae: 2233.5964\n",
      "Epoch 81/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23426540.0000 - mse: 23426540.0000 - mae: 2419.2295 - val_loss: 18985554.0000 - val_mse: 18985554.0000 - val_mae: 2162.1689\n",
      "Epoch 82/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23310322.0000 - mse: 23310322.0000 - mae: 2372.5918 - val_loss: 18877786.0000 - val_mse: 18877786.0000 - val_mae: 2175.6748\n",
      "Epoch 83/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 23182296.0000 - mse: 23182296.0000 - mae: 2353.4583 - val_loss: 18783212.0000 - val_mse: 18783212.0000 - val_mae: 2181.8350\n",
      "Epoch 84/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23074108.0000 - mse: 23074108.0000 - mae: 2359.7212 - val_loss: 18660596.0000 - val_mse: 18660596.0000 - val_mae: 2130.5164\n",
      "Epoch 85/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22978076.0000 - mse: 22978076.0000 - mae: 2328.8450 - val_loss: 18582614.0000 - val_mse: 18582614.0000 - val_mae: 2145.3884\n",
      "Epoch 86/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22888284.0000 - mse: 22888284.0000 - mae: 2333.3308 - val_loss: 18480176.0000 - val_mse: 18480176.0000 - val_mae: 2108.0076\n",
      "Epoch 87/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22806208.0000 - mse: 22806208.0000 - mae: 2303.0098 - val_loss: 18437960.0000 - val_mse: 18437958.0000 - val_mae: 2154.3345\n",
      "Epoch 88/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22734466.0000 - mse: 22734464.0000 - mae: 2337.4224 - val_loss: 18333216.0000 - val_mse: 18333216.0000 - val_mae: 2093.6194\n",
      "Epoch 89/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22663128.0000 - mse: 22663128.0000 - mae: 2277.6147 - val_loss: 18295756.0000 - val_mse: 18295756.0000 - val_mae: 2136.2139\n",
      "Epoch 90/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22618310.0000 - mse: 22618310.0000 - mae: 2310.9495 - val_loss: 18204930.0000 - val_mse: 18204930.0000 - val_mae: 2076.2256\n",
      "Epoch 91/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22559342.0000 - mse: 22559342.0000 - mae: 2255.4082 - val_loss: 18195640.0000 - val_mse: 18195640.0000 - val_mae: 2126.3369\n",
      "Epoch 92/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22510152.0000 - mse: 22510152.0000 - mae: 2281.9690 - val_loss: 18116534.0000 - val_mse: 18116534.0000 - val_mae: 2082.0559\n",
      "Epoch 93/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22459976.0000 - mse: 22459976.0000 - mae: 2263.6025 - val_loss: 18082954.0000 - val_mse: 18082954.0000 - val_mae: 2090.5300\n",
      "Epoch 94/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22426194.0000 - mse: 22426194.0000 - mae: 2260.0564 - val_loss: 18051348.0000 - val_mse: 18051348.0000 - val_mae: 2092.0598\n",
      "Epoch 95/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22393838.0000 - mse: 22393840.0000 - mae: 2259.9248 - val_loss: 17986914.0000 - val_mse: 17986914.0000 - val_mae: 2041.2600\n",
      "Epoch 96/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22370400.0000 - mse: 22370400.0000 - mae: 2230.6633 - val_loss: 17967098.0000 - val_mse: 17967098.0000 - val_mae: 2051.9094\n",
      "Epoch 97/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22339104.0000 - mse: 22339104.0000 - mae: 2258.2241 - val_loss: 17933586.0000 - val_mse: 17933586.0000 - val_mae: 2042.8203\n",
      "Epoch 98/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22322452.0000 - mse: 22322452.0000 - mae: 2222.6770 - val_loss: 17919398.0000 - val_mse: 17919398.0000 - val_mae: 2055.1338\n",
      "Epoch 99/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22304778.0000 - mse: 22304778.0000 - mae: 2239.6118 - val_loss: 17881610.0000 - val_mse: 17881610.0000 - val_mae: 2016.5896\n",
      "Epoch 100/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22283312.0000 - mse: 22283312.0000 - mae: 2225.7913 - val_loss: 17855170.0000 - val_mse: 17855172.0000 - val_mae: 1995.5178\n",
      "Epoch 101/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22267722.0000 - mse: 22267722.0000 - mae: 2212.3611 - val_loss: 17860780.0000 - val_mse: 17860780.0000 - val_mae: 2050.3928\n",
      "Epoch 102/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22252870.0000 - mse: 22252870.0000 - mae: 2219.6985 - val_loss: 17831080.0000 - val_mse: 17831080.0000 - val_mae: 2019.7761\n",
      "Epoch 103/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22233414.0000 - mse: 22233414.0000 - mae: 2189.1335 - val_loss: 17869720.0000 - val_mse: 17869720.0000 - val_mae: 2094.7410\n",
      "Epoch 104/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22234886.0000 - mse: 22234886.0000 - mae: 2236.7610 - val_loss: 17813260.0000 - val_mse: 17813260.0000 - val_mae: 2042.6959\n",
      "Epoch 105/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22215048.0000 - mse: 22215048.0000 - mae: 2200.0720 - val_loss: 17801244.0000 - val_mse: 17801244.0000 - val_mae: 2040.3206\n",
      "Epoch 106/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22207002.0000 - mse: 22207002.0000 - mae: 2196.5298 - val_loss: 17778668.0000 - val_mse: 17778668.0000 - val_mae: 2019.3180\n",
      "Epoch 107/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22196764.0000 - mse: 22196764.0000 - mae: 2222.8862 - val_loss: 17761656.0000 - val_mse: 17761656.0000 - val_mae: 2007.1892\n",
      "Epoch 108/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22191256.0000 - mse: 22191256.0000 - mae: 2192.2793 - val_loss: 17742458.0000 - val_mse: 17742458.0000 - val_mae: 1981.2518\n",
      "Epoch 109/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22182872.0000 - mse: 22182868.0000 - mae: 2189.8105 - val_loss: 17785004.0000 - val_mse: 17785004.0000 - val_mae: 2065.7622\n",
      "Epoch 110/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22177064.0000 - mse: 22177064.0000 - mae: 2212.9597 - val_loss: 17731324.0000 - val_mse: 17731324.0000 - val_mae: 1998.3690\n",
      "Epoch 111/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22165888.0000 - mse: 22165888.0000 - mae: 2183.6025 - val_loss: 17738362.0000 - val_mse: 17738364.0000 - val_mae: 2023.3976\n",
      "Epoch 112/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22161400.0000 - mse: 22161400.0000 - mae: 2201.8389 - val_loss: 17707466.0000 - val_mse: 17707466.0000 - val_mae: 1983.2416\n",
      "Epoch 113/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22149546.0000 - mse: 22149546.0000 - mae: 2165.6846 - val_loss: 17727722.0000 - val_mse: 17727722.0000 - val_mae: 2032.4016\n",
      "Epoch 114/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22146952.0000 - mse: 22146952.0000 - mae: 2181.7795 - val_loss: 17729798.0000 - val_mse: 17729798.0000 - val_mae: 2041.3896\n",
      "Epoch 115/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22143430.0000 - mse: 22143430.0000 - mae: 2207.1050 - val_loss: 17689648.0000 - val_mse: 17689648.0000 - val_mae: 1986.5144\n",
      "Epoch 116/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22133042.0000 - mse: 22133042.0000 - mae: 2168.4683 - val_loss: 17705508.0000 - val_mse: 17705508.0000 - val_mae: 2026.7133\n",
      "Epoch 117/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22140604.0000 - mse: 22140604.0000 - mae: 2190.9951 - val_loss: 17685986.0000 - val_mse: 17685986.0000 - val_mae: 2002.1478\n",
      "Epoch 118/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22135710.0000 - mse: 22135710.0000 - mae: 2190.4407 - val_loss: 17665596.0000 - val_mse: 17665596.0000 - val_mae: 1975.7867\n",
      "Epoch 119/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22120948.0000 - mse: 22120948.0000 - mae: 2180.9675 - val_loss: 17668134.0000 - val_mse: 17668134.0000 - val_mae: 1993.0752\n",
      "Epoch 120/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22113792.0000 - mse: 22113792.0000 - mae: 2178.9795 - val_loss: 17655242.0000 - val_mse: 17655242.0000 - val_mae: 1962.0739\n",
      "Epoch 121/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22109112.0000 - mse: 22109112.0000 - mae: 2178.5464 - val_loss: 17666450.0000 - val_mse: 17666450.0000 - val_mae: 2004.3947\n",
      "Epoch 122/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22103828.0000 - mse: 22103828.0000 - mae: 2154.2893 - val_loss: 17671014.0000 - val_mse: 17671014.0000 - val_mae: 2013.7025\n",
      "Epoch 123/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22101784.0000 - mse: 22101784.0000 - mae: 2196.2319 - val_loss: 17640662.0000 - val_mse: 17640662.0000 - val_mae: 1970.3781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22093502.0000 - mse: 22093502.0000 - mae: 2160.2463 - val_loss: 17657992.0000 - val_mse: 17657992.0000 - val_mae: 2010.3652\n",
      "Epoch 125/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22086108.0000 - mse: 22086108.0000 - mae: 2186.3547 - val_loss: 17627992.0000 - val_mse: 17627992.0000 - val_mae: 1962.8887\n",
      "Epoch 126/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22092792.0000 - mse: 22092792.0000 - mae: 2167.6306 - val_loss: 17636692.0000 - val_mse: 17636692.0000 - val_mae: 1988.6044\n",
      "Epoch 127/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22075046.0000 - mse: 22075044.0000 - mae: 2178.3706 - val_loss: 17613292.0000 - val_mse: 17613292.0000 - val_mae: 1923.4045\n",
      "Epoch 128/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22077954.0000 - mse: 22077954.0000 - mae: 2145.8953 - val_loss: 17640154.0000 - val_mse: 17640154.0000 - val_mae: 2008.3590\n",
      "Epoch 129/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22068602.0000 - mse: 22068602.0000 - mae: 2177.0698 - val_loss: 17618604.0000 - val_mse: 17618604.0000 - val_mae: 1980.6165\n",
      "Epoch 130/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22066068.0000 - mse: 22066068.0000 - mae: 2158.6113 - val_loss: 17634788.0000 - val_mse: 17634788.0000 - val_mae: 2012.1365\n",
      "Epoch 131/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22058116.0000 - mse: 22058116.0000 - mae: 2184.8093 - val_loss: 17596854.0000 - val_mse: 17596854.0000 - val_mae: 1918.4116\n",
      "Epoch 132/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22061564.0000 - mse: 22061564.0000 - mae: 2146.1177 - val_loss: 17615504.0000 - val_mse: 17615506.0000 - val_mae: 1988.7725\n",
      "Epoch 133/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22053164.0000 - mse: 22053164.0000 - mae: 2160.6748 - val_loss: 17601380.0000 - val_mse: 17601380.0000 - val_mae: 1967.2242\n",
      "Epoch 134/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22049536.0000 - mse: 22049536.0000 - mae: 2180.1128 - val_loss: 17587446.0000 - val_mse: 17587446.0000 - val_mae: 1946.7062\n",
      "Epoch 135/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22037456.0000 - mse: 22037456.0000 - mae: 2138.0769 - val_loss: 17624158.0000 - val_mse: 17624158.0000 - val_mae: 2019.0332\n",
      "Epoch 136/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22031540.0000 - mse: 22031540.0000 - mae: 2153.7937 - val_loss: 17635308.0000 - val_mse: 17635308.0000 - val_mae: 2033.9100\n",
      "Epoch 137/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22038830.0000 - mse: 22038830.0000 - mae: 2187.6667 - val_loss: 17578936.0000 - val_mse: 17578936.0000 - val_mae: 1952.3239\n",
      "Epoch 138/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22022932.0000 - mse: 22022932.0000 - mae: 2149.6179 - val_loss: 17594252.0000 - val_mse: 17594252.0000 - val_mae: 1990.2416\n",
      "Epoch 139/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22016944.0000 - mse: 22016944.0000 - mae: 2163.7952 - val_loss: 17569882.0000 - val_mse: 17569882.0000 - val_mae: 1957.7098\n",
      "Epoch 140/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22009518.0000 - mse: 22009518.0000 - mae: 2160.4360 - val_loss: 17563018.0000 - val_mse: 17563018.0000 - val_mae: 1949.0870\n",
      "Epoch 141/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22008800.0000 - mse: 22008800.0000 - mae: 2149.8945 - val_loss: 17565458.0000 - val_mse: 17565458.0000 - val_mae: 1963.2168\n",
      "Epoch 142/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22005140.0000 - mse: 22005140.0000 - mae: 2156.2002 - val_loss: 17557862.0000 - val_mse: 17557862.0000 - val_mae: 1955.7006\n",
      "Epoch 143/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21998598.0000 - mse: 21998598.0000 - mae: 2155.7009 - val_loss: 17562762.0000 - val_mse: 17562762.0000 - val_mae: 1975.3181\n",
      "Epoch 144/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21993298.0000 - mse: 21993298.0000 - mae: 2151.5913 - val_loss: 17557946.0000 - val_mse: 17557946.0000 - val_mae: 1972.8043\n",
      "Epoch 145/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21987708.0000 - mse: 21987708.0000 - mae: 2160.6089 - val_loss: 17538610.0000 - val_mse: 17538610.0000 - val_mae: 1925.4086\n",
      "Epoch 146/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21970784.0000 - mse: 21970782.0000 - mae: 2132.2932 - val_loss: 17584668.0000 - val_mse: 17584668.0000 - val_mae: 2020.7180\n",
      "Epoch 147/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21975338.0000 - mse: 21975338.0000 - mae: 2171.3848 - val_loss: 17543048.0000 - val_mse: 17543048.0000 - val_mae: 1965.3368\n",
      "Epoch 148/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21966598.0000 - mse: 21966598.0000 - mae: 2144.4658 - val_loss: 17561796.0000 - val_mse: 17561796.0000 - val_mae: 1997.8612\n",
      "Epoch 149/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21970680.0000 - mse: 21970682.0000 - mae: 2164.7852 - val_loss: 17546830.0000 - val_mse: 17546830.0000 - val_mae: 1983.5571\n",
      "Epoch 150/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21965360.0000 - mse: 21965360.0000 - mae: 2150.7534 - val_loss: 17532300.0000 - val_mse: 17532300.0000 - val_mae: 1963.8937\n",
      "Epoch 151/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21957636.0000 - mse: 21957636.0000 - mae: 2146.2993 - val_loss: 17520220.0000 - val_mse: 17520220.0000 - val_mae: 1948.5773\n",
      "Epoch 152/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21950220.0000 - mse: 21950220.0000 - mae: 2139.5879 - val_loss: 17531462.0000 - val_mse: 17531462.0000 - val_mae: 1976.6918\n",
      "Epoch 153/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21942796.0000 - mse: 21942796.0000 - mae: 2154.3047 - val_loss: 17526870.0000 - val_mse: 17526870.0000 - val_mae: 1973.6359\n",
      "Epoch 154/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21932802.0000 - mse: 21932802.0000 - mae: 2140.7974 - val_loss: 17523262.0000 - val_mse: 17523262.0000 - val_mae: 1973.9197\n",
      "Epoch 155/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21932822.0000 - mse: 21932822.0000 - mae: 2146.6372 - val_loss: 17559792.0000 - val_mse: 17559792.0000 - val_mae: 2025.6493\n",
      "Epoch 156/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21917890.0000 - mse: 21917890.0000 - mae: 2163.1868 - val_loss: 17496916.0000 - val_mse: 17496916.0000 - val_mae: 1928.9246\n",
      "Epoch 157/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21916876.0000 - mse: 21916876.0000 - mae: 2142.4253 - val_loss: 17496336.0000 - val_mse: 17496336.0000 - val_mae: 1944.9629\n",
      "Epoch 158/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21906836.0000 - mse: 21906838.0000 - mae: 2149.1343 - val_loss: 17483934.0000 - val_mse: 17483934.0000 - val_mae: 1920.4730\n",
      "Epoch 159/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21900098.0000 - mse: 21900100.0000 - mae: 2150.6196 - val_loss: 17480622.0000 - val_mse: 17480622.0000 - val_mae: 1925.9841\n",
      "Epoch 160/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21896144.0000 - mse: 21896144.0000 - mae: 2138.8301 - val_loss: 17478034.0000 - val_mse: 17478034.0000 - val_mae: 1926.3860\n",
      "Epoch 161/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21887022.0000 - mse: 21887022.0000 - mae: 2135.4485 - val_loss: 17497790.0000 - val_mse: 17497790.0000 - val_mae: 1980.1104\n",
      "Epoch 162/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21876186.0000 - mse: 21876186.0000 - mae: 2139.2222 - val_loss: 17495950.0000 - val_mse: 17495950.0000 - val_mae: 1980.9756\n",
      "Epoch 163/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21870934.0000 - mse: 21870934.0000 - mae: 2158.0540 - val_loss: 17463972.0000 - val_mse: 17463972.0000 - val_mae: 1921.4039\n",
      "Epoch 164/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21870376.0000 - mse: 21870376.0000 - mae: 2134.3020 - val_loss: 17470960.0000 - val_mse: 17470960.0000 - val_mae: 1950.6714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21862862.0000 - mse: 21862862.0000 - mae: 2141.7993 - val_loss: 17453768.0000 - val_mse: 17453768.0000 - val_mae: 1911.7737\n",
      "Epoch 166/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21858920.0000 - mse: 21858920.0000 - mae: 2130.0979 - val_loss: 17461646.0000 - val_mse: 17461646.0000 - val_mae: 1948.5486\n",
      "Epoch 167/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21850394.0000 - mse: 21850394.0000 - mae: 2149.9834 - val_loss: 17460410.0000 - val_mse: 17460410.0000 - val_mae: 1956.9708\n",
      "Epoch 168/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21839208.0000 - mse: 21839208.0000 - mae: 2151.2798 - val_loss: 17440326.0000 - val_mse: 17440326.0000 - val_mae: 1909.3264\n",
      "Epoch 169/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21835122.0000 - mse: 21835122.0000 - mae: 2114.8801 - val_loss: 17468038.0000 - val_mse: 17468038.0000 - val_mae: 1980.5763\n",
      "Epoch 170/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21828604.0000 - mse: 21828604.0000 - mae: 2140.5098 - val_loss: 17456468.0000 - val_mse: 17456466.0000 - val_mae: 1968.7944\n",
      "Epoch 171/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21817532.0000 - mse: 21817532.0000 - mae: 2125.8853 - val_loss: 17454686.0000 - val_mse: 17454686.0000 - val_mae: 1972.1797\n",
      "Epoch 172/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21814588.0000 - mse: 21814588.0000 - mae: 2141.0322 - val_loss: 17473008.0000 - val_mse: 17473008.0000 - val_mae: 2003.3654\n",
      "Epoch 173/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21810002.0000 - mse: 21810002.0000 - mae: 2143.5227 - val_loss: 17464230.0000 - val_mse: 17464230.0000 - val_mae: 1995.4766\n",
      "Epoch 174/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21794882.0000 - mse: 21794882.0000 - mae: 2141.9744 - val_loss: 17425078.0000 - val_mse: 17425078.0000 - val_mae: 1939.2115\n",
      "Epoch 175/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21786920.0000 - mse: 21786920.0000 - mae: 2125.4226 - val_loss: 17432528.0000 - val_mse: 17432528.0000 - val_mae: 1962.8104\n",
      "Epoch 176/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21781446.0000 - mse: 21781446.0000 - mae: 2144.4812 - val_loss: 17413940.0000 - val_mse: 17413940.0000 - val_mae: 1938.5552\n",
      "Epoch 177/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21776520.0000 - mse: 21776520.0000 - mae: 2130.9880 - val_loss: 17410144.0000 - val_mse: 17410144.0000 - val_mae: 1936.6119\n",
      "Epoch 178/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21767214.0000 - mse: 21767214.0000 - mae: 2124.6787 - val_loss: 17410918.0000 - val_mse: 17410918.0000 - val_mae: 1944.8167\n",
      "Epoch 179/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21757918.0000 - mse: 21757918.0000 - mae: 2135.9045 - val_loss: 17403030.0000 - val_mse: 17403030.0000 - val_mae: 1946.0764\n",
      "Epoch 180/250\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 21747808.0000 - mse: 21747808.0000 - mae: 2127.8240 - val_loss: 17434896.0000 - val_mse: 17434896.0000 - val_mae: 1996.1216\n",
      "Epoch 181/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21744192.0000 - mse: 21744192.0000 - mae: 2135.0645 - val_loss: 17391614.0000 - val_mse: 17391614.0000 - val_mae: 1935.9249\n",
      "Epoch 182/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21733090.0000 - mse: 21733090.0000 - mae: 2142.9766 - val_loss: 17377134.0000 - val_mse: 17377134.0000 - val_mae: 1905.4159\n",
      "Epoch 183/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21730340.0000 - mse: 21730340.0000 - mae: 2130.7317 - val_loss: 17371300.0000 - val_mse: 17371300.0000 - val_mae: 1886.9841\n",
      "Epoch 184/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21722852.0000 - mse: 21722852.0000 - mae: 2112.8362 - val_loss: 17378286.0000 - val_mse: 17378286.0000 - val_mae: 1937.0409\n",
      "Epoch 185/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21714462.0000 - mse: 21714458.0000 - mae: 2123.0977 - val_loss: 17384568.0000 - val_mse: 17384568.0000 - val_mae: 1954.9786\n",
      "Epoch 186/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21705318.0000 - mse: 21705318.0000 - mae: 2122.8130 - val_loss: 17374624.0000 - val_mse: 17374624.0000 - val_mae: 1940.3718\n",
      "Epoch 187/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21697106.0000 - mse: 21697106.0000 - mae: 2118.9617 - val_loss: 17409734.0000 - val_mse: 17409734.0000 - val_mae: 1996.6317\n",
      "Epoch 188/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21693208.0000 - mse: 21693208.0000 - mae: 2141.6638 - val_loss: 17367338.0000 - val_mse: 17367338.0000 - val_mae: 1950.4135\n",
      "Epoch 189/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21688546.0000 - mse: 21688546.0000 - mae: 2117.3123 - val_loss: 17358102.0000 - val_mse: 17358102.0000 - val_mae: 1936.7168\n",
      "Epoch 190/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21673854.0000 - mse: 21673854.0000 - mae: 2118.3091 - val_loss: 17357588.0000 - val_mse: 17357588.0000 - val_mae: 1947.9814\n",
      "Epoch 191/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21664792.0000 - mse: 21664792.0000 - mae: 2129.4319 - val_loss: 17349508.0000 - val_mse: 17349508.0000 - val_mae: 1939.0905\n",
      "Epoch 192/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21655180.0000 - mse: 21655180.0000 - mae: 2119.7158 - val_loss: 17368726.0000 - val_mse: 17368726.0000 - val_mae: 1977.3446\n",
      "Epoch 193/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21645362.0000 - mse: 21645362.0000 - mae: 2128.2029 - val_loss: 17335368.0000 - val_mse: 17335368.0000 - val_mae: 1924.8281\n",
      "Epoch 194/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21639210.0000 - mse: 21639210.0000 - mae: 2124.2900 - val_loss: 17324142.0000 - val_mse: 17324142.0000 - val_mae: 1912.0862\n",
      "Epoch 195/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21627318.0000 - mse: 21627318.0000 - mae: 2128.4893 - val_loss: 17319866.0000 - val_mse: 17319866.0000 - val_mae: 1913.0597\n",
      "Epoch 196/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21618794.0000 - mse: 21618792.0000 - mae: 2106.4329 - val_loss: 17365016.0000 - val_mse: 17365016.0000 - val_mae: 1991.4827\n",
      "Epoch 197/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21623452.0000 - mse: 21623452.0000 - mae: 2118.7319 - val_loss: 17331638.0000 - val_mse: 17331638.0000 - val_mae: 1953.1851\n",
      "Epoch 198/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21604634.0000 - mse: 21604634.0000 - mae: 2129.5754 - val_loss: 17320340.0000 - val_mse: 17320340.0000 - val_mae: 1947.2590\n",
      "Epoch 199/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21598680.0000 - mse: 21598680.0000 - mae: 2121.3274 - val_loss: 17315916.0000 - val_mse: 17315916.0000 - val_mae: 1943.9054\n",
      "Epoch 200/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21588360.0000 - mse: 21588360.0000 - mae: 2113.7637 - val_loss: 17347242.0000 - val_mse: 17347242.0000 - val_mae: 1991.7832\n",
      "Epoch 201/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21582246.0000 - mse: 21582246.0000 - mae: 2118.7322 - val_loss: 17324206.0000 - val_mse: 17324206.0000 - val_mae: 1968.7079\n",
      "Epoch 202/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21571228.0000 - mse: 21571228.0000 - mae: 2120.8884 - val_loss: 17286520.0000 - val_mse: 17286520.0000 - val_mae: 1914.9685\n",
      "Epoch 203/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21563660.0000 - mse: 21563660.0000 - mae: 2115.8418 - val_loss: 17279360.0000 - val_mse: 17279360.0000 - val_mae: 1900.1816\n",
      "Epoch 204/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21558120.0000 - mse: 21558120.0000 - mae: 2118.6113 - val_loss: 17298616.0000 - val_mse: 17298616.0000 - val_mae: 1950.7347\n",
      "Epoch 205/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21547262.0000 - mse: 21547262.0000 - mae: 2112.2876 - val_loss: 17293036.0000 - val_mse: 17293036.0000 - val_mae: 1948.8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21536290.0000 - mse: 21536290.0000 - mae: 2103.3330 - val_loss: 17286964.0000 - val_mse: 17286964.0000 - val_mae: 1943.7709\n",
      "Epoch 207/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21532466.0000 - mse: 21532466.0000 - mae: 2118.5554 - val_loss: 17271466.0000 - val_mse: 17271466.0000 - val_mae: 1920.9462\n",
      "Epoch 208/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21515386.0000 - mse: 21515388.0000 - mae: 2104.3201 - val_loss: 17316612.0000 - val_mse: 17316612.0000 - val_mae: 1992.7759\n",
      "Epoch 209/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21510502.0000 - mse: 21510502.0000 - mae: 2111.7952 - val_loss: 17286962.0000 - val_mse: 17286962.0000 - val_mae: 1964.1858\n",
      "Epoch 210/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21503708.0000 - mse: 21503708.0000 - mae: 2126.8218 - val_loss: 17250328.0000 - val_mse: 17250328.0000 - val_mae: 1903.1356\n",
      "Epoch 211/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21494678.0000 - mse: 21494678.0000 - mae: 2094.1614 - val_loss: 17297632.0000 - val_mse: 17297632.0000 - val_mae: 1984.5933\n",
      "Epoch 212/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21495378.0000 - mse: 21495378.0000 - mae: 2125.3542 - val_loss: 17239524.0000 - val_mse: 17239524.0000 - val_mae: 1902.9832\n",
      "Epoch 213/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21477946.0000 - mse: 21477946.0000 - mae: 2100.6846 - val_loss: 17266994.0000 - val_mse: 17266994.0000 - val_mae: 1958.4839\n",
      "Epoch 214/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21469270.0000 - mse: 21469270.0000 - mae: 2125.2974 - val_loss: 17234158.0000 - val_mse: 17234158.0000 - val_mae: 1907.2522\n",
      "Epoch 215/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21468440.0000 - mse: 21468440.0000 - mae: 2097.9990 - val_loss: 17246600.0000 - val_mse: 17246600.0000 - val_mae: 1935.3789\n",
      "Epoch 216/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21453418.0000 - mse: 21453418.0000 - mae: 2112.2092 - val_loss: 17239090.0000 - val_mse: 17239090.0000 - val_mae: 1925.3820\n",
      "Epoch 217/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21444986.0000 - mse: 21444986.0000 - mae: 2106.7263 - val_loss: 17234112.0000 - val_mse: 17234112.0000 - val_mae: 1926.1465\n",
      "Epoch 218/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21437110.0000 - mse: 21437110.0000 - mae: 2110.5173 - val_loss: 17221710.0000 - val_mse: 17221710.0000 - val_mae: 1908.2407\n",
      "Epoch 219/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21428520.0000 - mse: 21428520.0000 - mae: 2089.1086 - val_loss: 17265860.0000 - val_mse: 17265860.0000 - val_mae: 1978.9185\n",
      "Epoch 220/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21429334.0000 - mse: 21429334.0000 - mae: 2103.6064 - val_loss: 17269546.0000 - val_mse: 17269546.0000 - val_mae: 1987.6315\n",
      "Epoch 221/250\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 21417694.0000 - mse: 21417694.0000 - mae: 2129.6235 - val_loss: 17208860.0000 - val_mse: 17208860.0000 - val_mae: 1900.4370\n",
      "Epoch 222/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21402578.0000 - mse: 21402578.0000 - mae: 2096.2725 - val_loss: 17231942.0000 - val_mse: 17231944.0000 - val_mae: 1955.4000\n",
      "Epoch 223/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21397728.0000 - mse: 21397728.0000 - mae: 2106.5918 - val_loss: 17221822.0000 - val_mse: 17221822.0000 - val_mae: 1945.3395\n",
      "Epoch 224/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21379760.0000 - mse: 21379760.0000 - mae: 2118.5479 - val_loss: 17191036.0000 - val_mse: 17191036.0000 - val_mae: 1848.9485\n",
      "Epoch 225/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21384434.0000 - mse: 21384434.0000 - mae: 2091.9976 - val_loss: 17209502.0000 - val_mse: 17209502.0000 - val_mae: 1936.1100\n",
      "Epoch 226/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21371268.0000 - mse: 21371268.0000 - mae: 2094.0510 - val_loss: 17215948.0000 - val_mse: 17215948.0000 - val_mae: 1951.2904\n",
      "Epoch 227/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21362922.0000 - mse: 21362922.0000 - mae: 2118.5129 - val_loss: 17181536.0000 - val_mse: 17181536.0000 - val_mae: 1899.8499\n",
      "Epoch 228/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21362838.0000 - mse: 21362838.0000 - mae: 2081.4319 - val_loss: 17196660.0000 - val_mse: 17196660.0000 - val_mae: 1931.9557\n",
      "Epoch 229/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21352958.0000 - mse: 21352958.0000 - mae: 2119.7700 - val_loss: 17183452.0000 - val_mse: 17183452.0000 - val_mae: 1914.7512\n",
      "Epoch 230/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21341780.0000 - mse: 21341780.0000 - mae: 2094.7603 - val_loss: 17182228.0000 - val_mse: 17182228.0000 - val_mae: 1916.8005\n",
      "Epoch 231/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21325306.0000 - mse: 21325306.0000 - mae: 2087.7048 - val_loss: 17234212.0000 - val_mse: 17234212.0000 - val_mae: 1989.3057\n",
      "Epoch 232/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21320712.0000 - mse: 21320712.0000 - mae: 2114.7349 - val_loss: 17166044.0000 - val_mse: 17166044.0000 - val_mae: 1900.1747\n",
      "Epoch 233/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21317344.0000 - mse: 21317344.0000 - mae: 2101.2698 - val_loss: 17160196.0000 - val_mse: 17160196.0000 - val_mae: 1868.3599\n",
      "Epoch 234/250\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21314698.0000 - mse: 21314698.0000 - mae: 2087.2910 - val_loss: 17173132.0000 - val_mse: 17173132.0000 - val_mae: 1925.5951\n",
      "Epoch 235/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21305104.0000 - mse: 21305104.0000 - mae: 2109.3140 - val_loss: 17171280.0000 - val_mse: 17171280.0000 - val_mae: 1926.9773\n",
      "Epoch 236/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21291082.0000 - mse: 21291082.0000 - mae: 2093.6711 - val_loss: 17150336.0000 - val_mse: 17150336.0000 - val_mae: 1895.5112\n",
      "Epoch 237/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21284428.0000 - mse: 21284428.0000 - mae: 2101.4990 - val_loss: 17168370.0000 - val_mse: 17168370.0000 - val_mae: 1935.0464\n",
      "Epoch 238/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21281066.0000 - mse: 21281066.0000 - mae: 2096.7769 - val_loss: 17146494.0000 - val_mse: 17146494.0000 - val_mae: 1899.7183\n",
      "Epoch 239/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21267692.0000 - mse: 21267692.0000 - mae: 2109.8560 - val_loss: 17139622.0000 - val_mse: 17139622.0000 - val_mae: 1883.3721\n",
      "Epoch 240/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21260848.0000 - mse: 21260848.0000 - mae: 2077.4636 - val_loss: 17219414.0000 - val_mse: 17219414.0000 - val_mae: 1999.5918\n",
      "Epoch 241/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21259274.0000 - mse: 21259274.0000 - mae: 2102.1450 - val_loss: 17161422.0000 - val_mse: 17161422.0000 - val_mae: 1939.0503\n",
      "Epoch 242/250\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 21251544.0000 - mse: 21251544.0000 - mae: 2101.7288 - val_loss: 17144512.0000 - val_mse: 17144512.0000 - val_mae: 1913.2201\n",
      "Epoch 243/250\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 21242534.0000 - mse: 21242534.0000 - mae: 2090.3547 - val_loss: 17154388.0000 - val_mse: 17154388.0000 - val_mae: 1934.1519\n",
      "Epoch 244/250\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 21233098.0000 - mse: 21233098.0000 - mae: 2099.4590 - val_loss: 17136356.0000 - val_mse: 17136356.0000 - val_mae: 1913.5598\n",
      "Epoch 245/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21227450.0000 - mse: 21227450.0000 - mae: 2090.3921 - val_loss: 17151604.0000 - val_mse: 17151604.0000 - val_mae: 1943.0210\n",
      "Epoch 246/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21217436.0000 - mse: 21217436.0000 - mae: 2106.4429 - val_loss: 17126234.0000 - val_mse: 17126234.0000 - val_mae: 1901.3682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21212294.0000 - mse: 21212294.0000 - mae: 2091.4702 - val_loss: 17122122.0000 - val_mse: 17122122.0000 - val_mae: 1884.7629\n",
      "Epoch 248/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21209242.0000 - mse: 21209242.0000 - mae: 2079.9961 - val_loss: 17152966.0000 - val_mse: 17152966.0000 - val_mae: 1948.0963\n",
      "Epoch 249/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21198358.0000 - mse: 21198360.0000 - mae: 2100.3459 - val_loss: 17127036.0000 - val_mse: 17127038.0000 - val_mae: 1917.4819\n",
      "Epoch 250/250\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21187262.0000 - mse: 21187262.0000 - mae: 2086.4041 - val_loss: 17170290.0000 - val_mse: 17170290.0000 - val_mae: 1975.2104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ac75a1250>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(3,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "model.fit(X_train, y_train, epochs=250, batch_size = 100, validation_data=(X_val, y_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d362d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step - loss: 30323706.0000 - mse: 30323706.0000 - mae: 2271.3337\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47bea204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the model predicts very badly, let try to stay D0-R24 features. The file was read again, Unnamed column was dropped and\n",
    "# Paralogs column turned out numerical.\n",
    "df = pd.read_csv(\"C:/Users/aisin/Desktop/Files/For Python/P. vanderplanki dataset/vanderplanki.csv\")\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df['Paralogs'] = df['Paralogs'].replace({'P':1, 'O':0})\n",
    "\n",
    "\n",
    "X, y = df[['D0', 'D24', 'D48', 'R3', 'R24', 'Paralogs', 'ExonN']], df['gene_length']\n",
    "\n",
    "scaled_df = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a59e063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val,  y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "164973ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/83\n",
      "109/109 [==============================] - 1s 4ms/step - loss: 41988320.0000 - mse: 41988320.0000 - mae: 3368.2642 - val_loss: 35374308.0000 - val_mse: 35374308.0000 - val_mae: 3151.3643\n",
      "Epoch 2/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 41571372.0000 - mse: 41571372.0000 - mae: 3304.5850 - val_loss: 34505760.0000 - val_mse: 34505760.0000 - val_mae: 3011.1763\n",
      "Epoch 3/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 39843856.0000 - mse: 39843856.0000 - mae: 3043.7898 - val_loss: 32103976.0000 - val_mse: 32103976.0000 - val_mae: 2614.2773\n",
      "Epoch 4/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 36655436.0000 - mse: 36655436.0000 - mae: 2621.5759 - val_loss: 28819846.0000 - val_mse: 28819846.0000 - val_mae: 2241.7527\n",
      "Epoch 5/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 33415876.0000 - mse: 33415876.0000 - mae: 2443.6509 - val_loss: 26521992.0000 - val_mse: 26521992.0000 - val_mae: 2261.9302\n",
      "Epoch 6/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31627788.0000 - mse: 31627788.0000 - mae: 2599.1528 - val_loss: 25791086.0000 - val_mse: 25791086.0000 - val_mae: 2471.7300\n",
      "Epoch 7/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31154026.0000 - mse: 31154026.0000 - mae: 2772.7102 - val_loss: 25709626.0000 - val_mse: 25709626.0000 - val_mae: 2577.7639\n",
      "Epoch 8/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31054954.0000 - mse: 31054954.0000 - mae: 2831.1853 - val_loss: 25683054.0000 - val_mse: 25683054.0000 - val_mae: 2620.1680\n",
      "Epoch 9/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31002616.0000 - mse: 31002616.0000 - mae: 2868.2236 - val_loss: 25629226.0000 - val_mse: 25629226.0000 - val_mae: 2606.6479\n",
      "Epoch 10/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30957656.0000 - mse: 30957656.0000 - mae: 2844.5581 - val_loss: 25601666.0000 - val_mse: 25601666.0000 - val_mae: 2628.4998\n",
      "Epoch 11/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30911720.0000 - mse: 30911720.0000 - mae: 2868.1196 - val_loss: 25548180.0000 - val_mse: 25548180.0000 - val_mae: 2606.9741\n",
      "Epoch 12/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30868750.0000 - mse: 30868750.0000 - mae: 2850.9280 - val_loss: 25515996.0000 - val_mse: 25515996.0000 - val_mae: 2618.8523\n",
      "Epoch 13/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30827134.0000 - mse: 30827134.0000 - mae: 2851.2798 - val_loss: 25482508.0000 - val_mse: 25482508.0000 - val_mae: 2624.5635\n",
      "Epoch 14/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30785610.0000 - mse: 30785610.0000 - mae: 2861.7139 - val_loss: 25442862.0000 - val_mse: 25442862.0000 - val_mae: 2621.3630\n",
      "Epoch 15/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30743672.0000 - mse: 30743672.0000 - mae: 2860.3020 - val_loss: 25404076.0000 - val_mse: 25404076.0000 - val_mae: 2620.5923\n",
      "Epoch 16/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30702924.0000 - mse: 30702924.0000 - mae: 2854.2703 - val_loss: 25371058.0000 - val_mse: 25371058.0000 - val_mae: 2623.2808\n",
      "Epoch 17/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30661288.0000 - mse: 30661288.0000 - mae: 2861.9775 - val_loss: 25323406.0000 - val_mse: 25323406.0000 - val_mae: 2605.5347\n",
      "Epoch 18/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30621970.0000 - mse: 30621970.0000 - mae: 2838.9636 - val_loss: 25290560.0000 - val_mse: 25290560.0000 - val_mae: 2609.6997\n",
      "Epoch 19/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30579474.0000 - mse: 30579474.0000 - mae: 2852.9116 - val_loss: 25270448.0000 - val_mse: 25270448.0000 - val_mae: 2631.8650\n",
      "Epoch 20/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30537046.0000 - mse: 30537046.0000 - mae: 2870.8127 - val_loss: 25223646.0000 - val_mse: 25223646.0000 - val_mae: 2620.1318\n",
      "Epoch 21/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30493730.0000 - mse: 30493730.0000 - mae: 2844.9285 - val_loss: 25184464.0000 - val_mse: 25184464.0000 - val_mae: 2614.2263\n",
      "Epoch 22/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30449256.0000 - mse: 30449256.0000 - mae: 2843.1365 - val_loss: 25149900.0000 - val_mse: 25149900.0000 - val_mae: 2619.9404\n",
      "Epoch 23/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30406194.0000 - mse: 30406194.0000 - mae: 2870.0647 - val_loss: 25100176.0000 - val_mse: 25100176.0000 - val_mae: 2603.6011\n",
      "Epoch 24/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30359320.0000 - mse: 30359320.0000 - mae: 2849.7214 - val_loss: 25059866.0000 - val_mse: 25059866.0000 - val_mae: 2602.1206\n",
      "Epoch 25/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30314416.0000 - mse: 30314416.0000 - mae: 2850.5515 - val_loss: 25021716.0000 - val_mse: 25021716.0000 - val_mae: 2602.2104\n",
      "Epoch 26/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30269328.0000 - mse: 30269328.0000 - mae: 2842.1387 - val_loss: 24982038.0000 - val_mse: 24982038.0000 - val_mae: 2604.1460\n",
      "Epoch 27/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30218576.0000 - mse: 30218572.0000 - mae: 2851.0110 - val_loss: 24944218.0000 - val_mse: 24944218.0000 - val_mae: 2607.7878\n",
      "Epoch 28/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30165458.0000 - mse: 30165458.0000 - mae: 2847.7134 - val_loss: 24890706.0000 - val_mse: 24890706.0000 - val_mae: 2593.9939\n",
      "Epoch 29/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30114052.0000 - mse: 30114052.0000 - mae: 2841.7698 - val_loss: 24856190.0000 - val_mse: 24856190.0000 - val_mae: 2609.2031\n",
      "Epoch 30/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30058640.0000 - mse: 30058640.0000 - mae: 2840.8694 - val_loss: 24807140.0000 - val_mse: 24807140.0000 - val_mae: 2604.3838\n",
      "Epoch 31/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30000166.0000 - mse: 30000166.0000 - mae: 2847.5217 - val_loss: 24753908.0000 - val_mse: 24753908.0000 - val_mae: 2596.0488\n",
      "Epoch 32/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29946454.0000 - mse: 29946454.0000 - mae: 2840.4680 - val_loss: 24698280.0000 - val_mse: 24698280.0000 - val_mae: 2582.2861\n",
      "Epoch 33/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29883086.0000 - mse: 29883086.0000 - mae: 2835.9480 - val_loss: 24653926.0000 - val_mse: 24653926.0000 - val_mae: 2593.9260\n",
      "Epoch 34/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29818874.0000 - mse: 29818874.0000 - mae: 2857.5508 - val_loss: 24584978.0000 - val_mse: 24584978.0000 - val_mae: 2564.6587\n",
      "Epoch 35/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29758258.0000 - mse: 29758258.0000 - mae: 2823.4680 - val_loss: 24528186.0000 - val_mse: 24528186.0000 - val_mae: 2557.4221\n",
      "Epoch 36/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29687914.0000 - mse: 29687914.0000 - mae: 2826.3020 - val_loss: 24483800.0000 - val_mse: 24483800.0000 - val_mae: 2584.0762\n",
      "Epoch 37/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29616342.0000 - mse: 29616342.0000 - mae: 2821.2124 - val_loss: 24424290.0000 - val_mse: 24424290.0000 - val_mae: 2580.7957\n",
      "Epoch 38/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29541112.0000 - mse: 29541112.0000 - mae: 2835.3184 - val_loss: 24362338.0000 - val_mse: 24362338.0000 - val_mae: 2580.7175\n",
      "Epoch 39/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29472298.0000 - mse: 29472298.0000 - mae: 2802.3506 - val_loss: 24325474.0000 - val_mse: 24325474.0000 - val_mae: 2612.2651\n",
      "Epoch 40/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29388028.0000 - mse: 29388028.0000 - mae: 2835.3494 - val_loss: 24245168.0000 - val_mse: 24245168.0000 - val_mae: 2592.3955\n",
      "Epoch 41/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29303660.0000 - mse: 29303660.0000 - mae: 2830.8352 - val_loss: 24163954.0000 - val_mse: 24163954.0000 - val_mae: 2571.5791\n",
      "Epoch 42/83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 29220040.0000 - mse: 29220040.0000 - mae: 2795.3242 - val_loss: 24101060.0000 - val_mse: 24101060.0000 - val_mae: 2581.7505\n",
      "Epoch 43/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29128070.0000 - mse: 29128070.0000 - mae: 2812.4873 - val_loss: 24021890.0000 - val_mse: 24021890.0000 - val_mae: 2574.8779\n",
      "Epoch 44/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29034836.0000 - mse: 29034836.0000 - mae: 2815.4724 - val_loss: 23937362.0000 - val_mse: 23937362.0000 - val_mae: 2561.6565\n",
      "Epoch 45/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28937936.0000 - mse: 28937936.0000 - mae: 2795.3667 - val_loss: 23862410.0000 - val_mse: 23862410.0000 - val_mae: 2568.6584\n",
      "Epoch 46/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28834270.0000 - mse: 28834270.0000 - mae: 2792.7776 - val_loss: 23791298.0000 - val_mse: 23791298.0000 - val_mae: 2583.9126\n",
      "Epoch 47/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28730706.0000 - mse: 28730702.0000 - mae: 2808.5449 - val_loss: 23671166.0000 - val_mse: 23671166.0000 - val_mae: 2524.4558\n",
      "Epoch 48/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28617588.0000 - mse: 28617588.0000 - mae: 2790.5222 - val_loss: 23584660.0000 - val_mse: 23584660.0000 - val_mae: 2532.1165\n",
      "Epoch 49/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28502636.0000 - mse: 28502636.0000 - mae: 2770.5864 - val_loss: 23513388.0000 - val_mse: 23513388.0000 - val_mae: 2566.6216\n",
      "Epoch 50/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28384194.0000 - mse: 28384194.0000 - mae: 2793.4265 - val_loss: 23395734.0000 - val_mse: 23395734.0000 - val_mae: 2528.6675\n",
      "Epoch 51/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28271164.0000 - mse: 28271164.0000 - mae: 2765.8521 - val_loss: 23301628.0000 - val_mse: 23301628.0000 - val_mae: 2534.7520\n",
      "Epoch 52/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28137500.0000 - mse: 28137500.0000 - mae: 2769.2964 - val_loss: 23188432.0000 - val_mse: 23188432.0000 - val_mae: 2512.9370\n",
      "Epoch 53/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28011034.0000 - mse: 28011034.0000 - mae: 2754.0000 - val_loss: 23092680.0000 - val_mse: 23092682.0000 - val_mae: 2528.1633\n",
      "Epoch 54/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27877084.0000 - mse: 27877084.0000 - mae: 2743.9761 - val_loss: 22994916.0000 - val_mse: 22994916.0000 - val_mae: 2538.3232\n",
      "Epoch 55/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27739892.0000 - mse: 27739892.0000 - mae: 2742.9089 - val_loss: 22870892.0000 - val_mse: 22870892.0000 - val_mae: 2518.1794\n",
      "Epoch 56/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27595120.0000 - mse: 27595120.0000 - mae: 2753.3108 - val_loss: 22742754.0000 - val_mse: 22742754.0000 - val_mae: 2496.1628\n",
      "Epoch 57/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27449460.0000 - mse: 27449460.0000 - mae: 2724.4839 - val_loss: 22625096.0000 - val_mse: 22625096.0000 - val_mae: 2495.3540\n",
      "Epoch 58/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 27301882.0000 - mse: 27301882.0000 - mae: 2724.6572 - val_loss: 22500028.0000 - val_mse: 22500028.0000 - val_mae: 2480.9817\n",
      "Epoch 59/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27148458.0000 - mse: 27148462.0000 - mae: 2712.0085 - val_loss: 22384786.0000 - val_mse: 22384786.0000 - val_mae: 2494.5439\n",
      "Epoch 60/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26994820.0000 - mse: 26994820.0000 - mae: 2698.4158 - val_loss: 22245942.0000 - val_mse: 22245942.0000 - val_mae: 2471.1636\n",
      "Epoch 61/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26833928.0000 - mse: 26833928.0000 - mae: 2688.8794 - val_loss: 22117162.0000 - val_mse: 22117162.0000 - val_mae: 2470.3499\n",
      "Epoch 62/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26668632.0000 - mse: 26668632.0000 - mae: 2673.6035 - val_loss: 21974502.0000 - val_mse: 21974502.0000 - val_mae: 2457.1743\n",
      "Epoch 63/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26499844.0000 - mse: 26499844.0000 - mae: 2666.5886 - val_loss: 21844794.0000 - val_mse: 21844794.0000 - val_mae: 2468.8101\n",
      "Epoch 64/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26322392.0000 - mse: 26322392.0000 - mae: 2666.1936 - val_loss: 21677242.0000 - val_mse: 21677242.0000 - val_mae: 2436.6887\n",
      "Epoch 65/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26142094.0000 - mse: 26142094.0000 - mae: 2637.4739 - val_loss: 21533102.0000 - val_mse: 21533102.0000 - val_mae: 2441.0977\n",
      "Epoch 66/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 25958900.0000 - mse: 25958900.0000 - mae: 2635.6580 - val_loss: 21364480.0000 - val_mse: 21364480.0000 - val_mae: 2414.7268\n",
      "Epoch 67/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25773032.0000 - mse: 25773032.0000 - mae: 2609.8687 - val_loss: 21212944.0000 - val_mse: 21212944.0000 - val_mae: 2418.7952\n",
      "Epoch 68/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 25578196.0000 - mse: 25578196.0000 - mae: 2615.8030 - val_loss: 21026144.0000 - val_mse: 21026142.0000 - val_mae: 2372.0525\n",
      "Epoch 69/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25392788.0000 - mse: 25392788.0000 - mae: 2578.7073 - val_loss: 20877270.0000 - val_mse: 20877272.0000 - val_mae: 2390.9705\n",
      "Epoch 70/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 25198124.0000 - mse: 25198124.0000 - mae: 2573.5715 - val_loss: 20708608.0000 - val_mse: 20708608.0000 - val_mae: 2377.4124\n",
      "Epoch 71/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25004422.0000 - mse: 25004422.0000 - mae: 2553.5081 - val_loss: 20517466.0000 - val_mse: 20517466.0000 - val_mae: 2331.0149\n",
      "Epoch 72/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24807168.0000 - mse: 24807168.0000 - mae: 2534.4224 - val_loss: 20348898.0000 - val_mse: 20348898.0000 - val_mae: 2325.6040\n",
      "Epoch 73/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 24610874.0000 - mse: 24610874.0000 - mae: 2526.3755 - val_loss: 20161988.0000 - val_mse: 20161988.0000 - val_mae: 2286.3274\n",
      "Epoch 74/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 24413612.0000 - mse: 24413612.0000 - mae: 2502.0029 - val_loss: 20034018.0000 - val_mse: 20034018.0000 - val_mae: 2343.4084\n",
      "Epoch 75/83\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 24234280.0000 - mse: 24234280.0000 - mae: 2504.1523 - val_loss: 19814728.0000 - val_mse: 19814728.0000 - val_mae: 2258.4812\n",
      "Epoch 76/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24038178.0000 - mse: 24038178.0000 - mae: 2480.0266 - val_loss: 19651812.0000 - val_mse: 19651812.0000 - val_mae: 2252.2458\n",
      "Epoch 77/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23856790.0000 - mse: 23856790.0000 - mae: 2463.6067 - val_loss: 19482094.0000 - val_mse: 19482094.0000 - val_mae: 2151.1450\n",
      "Epoch 78/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23689508.0000 - mse: 23689508.0000 - mae: 2433.5491 - val_loss: 19326858.0000 - val_mse: 19326858.0000 - val_mae: 2180.7451\n",
      "Epoch 79/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23522916.0000 - mse: 23522916.0000 - mae: 2420.0586 - val_loss: 19174700.0000 - val_mse: 19174700.0000 - val_mae: 2161.3174\n",
      "Epoch 80/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23361402.0000 - mse: 23361402.0000 - mae: 2411.3042 - val_loss: 19032594.0000 - val_mse: 19032594.0000 - val_mae: 2152.3315\n",
      "Epoch 81/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23228970.0000 - mse: 23228970.0000 - mae: 2375.4136 - val_loss: 18914864.0000 - val_mse: 18914864.0000 - val_mae: 2180.6311\n",
      "Epoch 82/83\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 23089506.0000 - mse: 23089506.0000 - mae: 2365.7129 - val_loss: 18807296.0000 - val_mse: 18807296.0000 - val_mae: 2211.5801\n",
      "Epoch 83/83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 22964320.0000 - mse: 22964320.0000 - mae: 2361.5669 - val_loss: 18666170.0000 - val_mse: 18666170.0000 - val_mae: 2075.6196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ad1f058e0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "model.fit(X_train, y_train, epochs=83, batch_size = 100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4f129ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step - loss: 32432412.0000 - mse: 32432412.0000 - mae: 2357.4653\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0e0fbab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 41985272.0000 - mse: 41985272.0000 - mae: 3368.0195 - val_loss: 35330012.0000 - val_mse: 35330008.0000 - val_mae: 3144.4490\n",
      "Epoch 2/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 40921144.0000 - mse: 40921144.0000 - mae: 3203.0847 - val_loss: 32682398.0000 - val_mse: 32682400.0000 - val_mae: 2706.5554\n",
      "Epoch 3/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 35444284.0000 - mse: 35444284.0000 - mae: 2577.4167 - val_loss: 26421286.0000 - val_mse: 26421286.0000 - val_mae: 2274.3699\n",
      "Epoch 4/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31266148.0000 - mse: 31266148.0000 - mae: 2737.6392 - val_loss: 25719364.0000 - val_mse: 25719364.0000 - val_mae: 2625.2661\n",
      "Epoch 5/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 31017938.0000 - mse: 31017938.0000 - mae: 2866.3701 - val_loss: 25635284.0000 - val_mse: 25635284.0000 - val_mae: 2624.3418\n",
      "Epoch 6/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30934512.0000 - mse: 30934512.0000 - mae: 2873.9106 - val_loss: 25543710.0000 - val_mse: 25543708.0000 - val_mae: 2610.5217\n",
      "Epoch 7/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30847198.0000 - mse: 30847198.0000 - mae: 2859.7471 - val_loss: 25471786.0000 - val_mse: 25471786.0000 - val_mae: 2621.5330\n",
      "Epoch 8/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30761722.0000 - mse: 30761722.0000 - mae: 2865.9958 - val_loss: 25379428.0000 - val_mse: 25379428.0000 - val_mae: 2598.3979\n",
      "Epoch 9/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30673590.0000 - mse: 30673590.0000 - mae: 2862.1401 - val_loss: 25302178.0000 - val_mse: 25302178.0000 - val_mae: 2604.1990\n",
      "Epoch 10/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30573228.0000 - mse: 30573228.0000 - mae: 2829.7402 - val_loss: 25248648.0000 - val_mse: 25248648.0000 - val_mae: 2636.1646\n",
      "Epoch 11/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30476904.0000 - mse: 30476904.0000 - mae: 2874.5027 - val_loss: 25122612.0000 - val_mse: 25122612.0000 - val_mae: 2573.7786\n",
      "Epoch 12/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 30386156.0000 - mse: 30386156.0000 - mae: 2831.4573 - val_loss: 25057914.0000 - val_mse: 25057914.0000 - val_mae: 2609.1169\n",
      "Epoch 13/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30279464.0000 - mse: 30279464.0000 - mae: 2854.0112 - val_loss: 24973980.0000 - val_mse: 24973980.0000 - val_mae: 2619.8660\n",
      "Epoch 14/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30163498.0000 - mse: 30163498.0000 - mae: 2840.8276 - val_loss: 24851276.0000 - val_mse: 24851276.0000 - val_mae: 2573.0005\n",
      "Epoch 15/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 30045284.0000 - mse: 30045284.0000 - mae: 2858.2810 - val_loss: 24755322.0000 - val_mse: 24755322.0000 - val_mae: 2592.4163\n",
      "Epoch 16/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29922520.0000 - mse: 29922520.0000 - mae: 2835.1367 - val_loss: 24622044.0000 - val_mse: 24622044.0000 - val_mae: 2544.4080\n",
      "Epoch 17/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 29779572.0000 - mse: 29779572.0000 - mae: 2832.5774 - val_loss: 24513268.0000 - val_mse: 24513268.0000 - val_mae: 2575.2246\n",
      "Epoch 18/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29626066.0000 - mse: 29626066.0000 - mae: 2813.1042 - val_loss: 24399646.0000 - val_mse: 24399646.0000 - val_mae: 2601.7874\n",
      "Epoch 19/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29451480.0000 - mse: 29451480.0000 - mae: 2829.5615 - val_loss: 24230584.0000 - val_mse: 24230584.0000 - val_mae: 2560.3772\n",
      "Epoch 20/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29267252.0000 - mse: 29267252.0000 - mae: 2801.1013 - val_loss: 24087750.0000 - val_mse: 24087750.0000 - val_mae: 2582.5959\n",
      "Epoch 21/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 29059374.0000 - mse: 29059374.0000 - mae: 2804.5579 - val_loss: 23939364.0000 - val_mse: 23939362.0000 - val_mae: 2614.6228\n",
      "Epoch 22/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 28837300.0000 - mse: 28837300.0000 - mae: 2796.9448 - val_loss: 23744592.0000 - val_mse: 23744592.0000 - val_mae: 2600.8542\n",
      "Epoch 23/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28582476.0000 - mse: 28582476.0000 - mae: 2786.7708 - val_loss: 23505272.0000 - val_mse: 23505272.0000 - val_mae: 2562.5396\n",
      "Epoch 24/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 28311530.0000 - mse: 28311530.0000 - mae: 2764.0234 - val_loss: 23290654.0000 - val_mse: 23290654.0000 - val_mae: 2567.7883\n",
      "Epoch 25/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 28016142.0000 - mse: 28016142.0000 - mae: 2773.5361 - val_loss: 23025288.0000 - val_mse: 23025288.0000 - val_mae: 2545.5569\n",
      "Epoch 26/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27683632.0000 - mse: 27683632.0000 - mae: 2727.0684 - val_loss: 22762846.0000 - val_mse: 22762846.0000 - val_mae: 2548.6477\n",
      "Epoch 27/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 27317288.0000 - mse: 27317288.0000 - mae: 2734.3337 - val_loss: 22417110.0000 - val_mse: 22417110.0000 - val_mae: 2469.7371\n",
      "Epoch 28/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 26943342.0000 - mse: 26943342.0000 - mae: 2697.7737 - val_loss: 22078360.0000 - val_mse: 22078360.0000 - val_mae: 2443.6445\n",
      "Epoch 29/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26512108.0000 - mse: 26512108.0000 - mae: 2666.3633 - val_loss: 21744944.0000 - val_mse: 21744944.0000 - val_mae: 2474.5410\n",
      "Epoch 30/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 26059024.0000 - mse: 26059024.0000 - mae: 2651.4568 - val_loss: 21320008.0000 - val_mse: 21320008.0000 - val_mae: 2391.3652\n",
      "Epoch 31/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25565336.0000 - mse: 25565336.0000 - mae: 2604.9084 - val_loss: 20894946.0000 - val_mse: 20894946.0000 - val_mae: 2403.8418\n",
      "Epoch 32/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 25036474.0000 - mse: 25036474.0000 - mae: 2562.4829 - val_loss: 20453602.0000 - val_mse: 20453602.0000 - val_mae: 2373.2090\n",
      "Epoch 33/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 24505462.0000 - mse: 24505462.0000 - mae: 2518.0708 - val_loss: 20045946.0000 - val_mse: 20045946.0000 - val_mae: 2407.9900\n",
      "Epoch 34/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 23962110.0000 - mse: 23962110.0000 - mae: 2466.6870 - val_loss: 19552714.0000 - val_mse: 19552714.0000 - val_mae: 2337.0911\n",
      "Epoch 35/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 23484364.0000 - mse: 23484364.0000 - mae: 2429.2427 - val_loss: 19042566.0000 - val_mse: 19042568.0000 - val_mae: 2167.6047\n",
      "Epoch 36/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 23053722.0000 - mse: 23053722.0000 - mae: 2399.1797 - val_loss: 18679912.0000 - val_mse: 18679912.0000 - val_mae: 2029.3733\n",
      "Epoch 37/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 22700464.0000 - mse: 22700464.0000 - mae: 2321.1604 - val_loss: 18417078.0000 - val_mse: 18417078.0000 - val_mae: 2193.2607\n",
      "Epoch 38/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22432732.0000 - mse: 22432732.0000 - mae: 2309.8975 - val_loss: 18150032.0000 - val_mse: 18150032.0000 - val_mae: 2121.8479\n",
      "Epoch 39/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22180766.0000 - mse: 22180766.0000 - mae: 2257.4604 - val_loss: 17953942.0000 - val_mse: 17953942.0000 - val_mae: 2082.3662\n",
      "Epoch 40/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 22025346.0000 - mse: 22025346.0000 - mae: 2247.3645 - val_loss: 17787148.0000 - val_mse: 17787148.0000 - val_mae: 1957.9789\n",
      "Epoch 41/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21931614.0000 - mse: 21931614.0000 - mae: 2203.1946 - val_loss: 17715076.0000 - val_mse: 17715076.0000 - val_mae: 2051.2546\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 3ms/step - loss: 21855900.0000 - mse: 21855900.0000 - mae: 2213.2786 - val_loss: 17648640.0000 - val_mse: 17648640.0000 - val_mae: 2044.1980\n",
      "Epoch 43/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21775824.0000 - mse: 21775824.0000 - mae: 2176.9399 - val_loss: 17574648.0000 - val_mse: 17574648.0000 - val_mae: 2000.6936\n",
      "Epoch 44/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21718218.0000 - mse: 21718218.0000 - mae: 2187.2026 - val_loss: 17518636.0000 - val_mse: 17518636.0000 - val_mae: 1956.0913\n",
      "Epoch 45/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21674146.0000 - mse: 21674146.0000 - mae: 2167.7034 - val_loss: 17473878.0000 - val_mse: 17473878.0000 - val_mae: 1952.7244\n",
      "Epoch 46/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21649258.0000 - mse: 21649258.0000 - mae: 2150.6626 - val_loss: 17470014.0000 - val_mse: 17470014.0000 - val_mae: 1991.6742\n",
      "Epoch 47/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21614892.0000 - mse: 21614892.0000 - mae: 2149.7737 - val_loss: 17467200.0000 - val_mse: 17467200.0000 - val_mae: 2014.6327\n",
      "Epoch 48/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21580232.0000 - mse: 21580232.0000 - mae: 2164.3062 - val_loss: 17435606.0000 - val_mse: 17435606.0000 - val_mae: 1863.5551\n",
      "Epoch 49/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21587744.0000 - mse: 21587744.0000 - mae: 2131.9990 - val_loss: 17397974.0000 - val_mse: 17397974.0000 - val_mae: 1981.1915\n",
      "Epoch 50/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21525594.0000 - mse: 21525594.0000 - mae: 2141.6055 - val_loss: 17388030.0000 - val_mse: 17388030.0000 - val_mae: 1987.6895\n",
      "Epoch 51/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21498564.0000 - mse: 21498564.0000 - mae: 2136.5901 - val_loss: 17350830.0000 - val_mse: 17350828.0000 - val_mae: 1920.4425\n",
      "Epoch 52/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21476714.0000 - mse: 21476714.0000 - mae: 2132.9277 - val_loss: 17357260.0000 - val_mse: 17357260.0000 - val_mae: 1992.7089\n",
      "Epoch 53/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21464634.0000 - mse: 21464634.0000 - mae: 2129.1604 - val_loss: 17381060.0000 - val_mse: 17381060.0000 - val_mae: 2027.1388\n",
      "Epoch 54/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21427940.0000 - mse: 21427940.0000 - mae: 2127.3875 - val_loss: 17286848.0000 - val_mse: 17286848.0000 - val_mae: 1935.0295\n",
      "Epoch 55/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21406056.0000 - mse: 21406056.0000 - mae: 2126.6841 - val_loss: 17286384.0000 - val_mse: 17286384.0000 - val_mae: 1936.1212\n",
      "Epoch 56/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21396152.0000 - mse: 21396152.0000 - mae: 2125.6289 - val_loss: 17274704.0000 - val_mse: 17274704.0000 - val_mae: 1913.9950\n",
      "Epoch 57/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21379382.0000 - mse: 21379382.0000 - mae: 2120.7798 - val_loss: 17261974.0000 - val_mse: 17261974.0000 - val_mae: 1918.0005\n",
      "Epoch 58/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21346924.0000 - mse: 21346924.0000 - mae: 2116.9934 - val_loss: 17275658.0000 - val_mse: 17275658.0000 - val_mae: 1962.1226\n",
      "Epoch 59/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21354634.0000 - mse: 21354634.0000 - mae: 2117.3557 - val_loss: 17239826.0000 - val_mse: 17239826.0000 - val_mae: 1877.8818\n",
      "Epoch 60/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21300342.0000 - mse: 21300342.0000 - mae: 2119.2942 - val_loss: 17219534.0000 - val_mse: 17219534.0000 - val_mae: 1889.4698\n",
      "Epoch 61/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21282586.0000 - mse: 21282586.0000 - mae: 2109.0146 - val_loss: 17211570.0000 - val_mse: 17211570.0000 - val_mae: 1914.8025\n",
      "Epoch 62/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21279950.0000 - mse: 21279950.0000 - mae: 2112.2688 - val_loss: 17262938.0000 - val_mse: 17262938.0000 - val_mae: 2020.4891\n",
      "Epoch 63/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21240722.0000 - mse: 21240722.0000 - mae: 2120.7915 - val_loss: 17180840.0000 - val_mse: 17180840.0000 - val_mae: 1911.2450\n",
      "Epoch 64/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21231146.0000 - mse: 21231146.0000 - mae: 2116.3547 - val_loss: 17186684.0000 - val_mse: 17186684.0000 - val_mae: 1954.5507\n",
      "Epoch 65/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21202782.0000 - mse: 21202782.0000 - mae: 2105.9592 - val_loss: 17169642.0000 - val_mse: 17169642.0000 - val_mae: 1929.8887\n",
      "Epoch 66/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21164104.0000 - mse: 21164104.0000 - mae: 2109.0991 - val_loss: 17165720.0000 - val_mse: 17165720.0000 - val_mae: 1931.7148\n",
      "Epoch 67/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21179222.0000 - mse: 21179222.0000 - mae: 2112.3127 - val_loss: 17260232.0000 - val_mse: 17260234.0000 - val_mae: 2051.4097\n",
      "Epoch 68/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21154578.0000 - mse: 21154576.0000 - mae: 2108.4170 - val_loss: 17150346.0000 - val_mse: 17150346.0000 - val_mae: 1923.6549\n",
      "Epoch 69/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21146332.0000 - mse: 21146332.0000 - mae: 2104.2542 - val_loss: 17128886.0000 - val_mse: 17128886.0000 - val_mae: 1881.9711\n",
      "Epoch 70/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21122198.0000 - mse: 21122198.0000 - mae: 2101.1899 - val_loss: 17163890.0000 - val_mse: 17163890.0000 - val_mae: 1972.5109\n",
      "Epoch 71/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21093464.0000 - mse: 21093464.0000 - mae: 2107.2825 - val_loss: 17161680.0000 - val_mse: 17161680.0000 - val_mae: 1985.7970\n",
      "Epoch 72/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21085836.0000 - mse: 21085836.0000 - mae: 2098.8159 - val_loss: 17233860.0000 - val_mse: 17233860.0000 - val_mae: 2041.9386\n",
      "Epoch 73/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21085828.0000 - mse: 21085828.0000 - mae: 2117.3948 - val_loss: 17185194.0000 - val_mse: 17185194.0000 - val_mae: 1817.7799\n",
      "Epoch 74/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21053284.0000 - mse: 21053284.0000 - mae: 2102.5054 - val_loss: 17107082.0000 - val_mse: 17107082.0000 - val_mae: 1934.7789\n",
      "Epoch 75/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21046778.0000 - mse: 21046778.0000 - mae: 2091.9966 - val_loss: 17104184.0000 - val_mse: 17104184.0000 - val_mae: 1926.2516\n",
      "Epoch 76/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21018424.0000 - mse: 21018424.0000 - mae: 2099.8982 - val_loss: 17093786.0000 - val_mse: 17093786.0000 - val_mae: 1869.9246\n",
      "Epoch 77/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 21025130.0000 - mse: 21025132.0000 - mae: 2098.1982 - val_loss: 17115558.0000 - val_mse: 17115558.0000 - val_mae: 1960.2981\n",
      "Epoch 78/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21008238.0000 - mse: 21008238.0000 - mae: 2097.1492 - val_loss: 17197342.0000 - val_mse: 17197342.0000 - val_mae: 2040.5364\n",
      "Epoch 79/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 21034692.0000 - mse: 21034692.0000 - mae: 2121.4263 - val_loss: 17099358.0000 - val_mse: 17099358.0000 - val_mae: 1838.4479\n",
      "Epoch 80/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20961706.0000 - mse: 20961708.0000 - mae: 2080.8584 - val_loss: 17202894.0000 - val_mse: 17202894.0000 - val_mae: 2046.0503\n",
      "Epoch 81/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20975114.0000 - mse: 20975114.0000 - mae: 2093.1965 - val_loss: 17146872.0000 - val_mse: 17146872.0000 - val_mae: 2013.5923\n",
      "Epoch 82/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20984408.0000 - mse: 20984410.0000 - mae: 2105.1262 - val_loss: 17099686.0000 - val_mse: 17099686.0000 - val_mae: 1969.3240\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 2ms/step - loss: 20938450.0000 - mse: 20938450.0000 - mae: 2089.1323 - val_loss: 17139972.0000 - val_mse: 17139972.0000 - val_mae: 2011.4749\n",
      "Epoch 84/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20948148.0000 - mse: 20948148.0000 - mae: 2098.9766 - val_loss: 17073896.0000 - val_mse: 17073896.0000 - val_mae: 1948.1099\n",
      "Epoch 85/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20913200.0000 - mse: 20913198.0000 - mae: 2092.5952 - val_loss: 17056970.0000 - val_mse: 17056970.0000 - val_mae: 1893.8687\n",
      "Epoch 86/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20906886.0000 - mse: 20906886.0000 - mae: 2097.6580 - val_loss: 17109814.0000 - val_mse: 17109814.0000 - val_mae: 1997.8677\n",
      "Epoch 87/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20916652.0000 - mse: 20916650.0000 - mae: 2096.6716 - val_loss: 17101414.0000 - val_mse: 17101414.0000 - val_mae: 1994.1617\n",
      "Epoch 88/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20886218.0000 - mse: 20886218.0000 - mae: 2085.5293 - val_loss: 17180162.0000 - val_mse: 17180162.0000 - val_mae: 2055.7627\n",
      "Epoch 89/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20875542.0000 - mse: 20875542.0000 - mae: 2103.8264 - val_loss: 17079688.0000 - val_mse: 17079688.0000 - val_mae: 1979.1434\n",
      "Epoch 90/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20870046.0000 - mse: 20870046.0000 - mae: 2081.6140 - val_loss: 17028636.0000 - val_mse: 17028636.0000 - val_mae: 1866.9611\n",
      "Epoch 91/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20868476.0000 - mse: 20868476.0000 - mae: 2089.3792 - val_loss: 17107454.0000 - val_mse: 17107454.0000 - val_mae: 2008.1362\n",
      "Epoch 92/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20852722.0000 - mse: 20852722.0000 - mae: 2088.4170 - val_loss: 17063970.0000 - val_mse: 17063968.0000 - val_mae: 1964.1099\n",
      "Epoch 93/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20845034.0000 - mse: 20845034.0000 - mae: 2092.6655 - val_loss: 17020322.0000 - val_mse: 17020322.0000 - val_mae: 1898.3312\n",
      "Epoch 94/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20825100.0000 - mse: 20825100.0000 - mae: 2097.1377 - val_loss: 17019220.0000 - val_mse: 17019220.0000 - val_mae: 1870.7197\n",
      "Epoch 95/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20821898.0000 - mse: 20821898.0000 - mae: 2083.6743 - val_loss: 17040366.0000 - val_mse: 17040364.0000 - val_mae: 1947.4207\n",
      "Epoch 96/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20846424.0000 - mse: 20846424.0000 - mae: 2090.2031 - val_loss: 17139880.0000 - val_mse: 17139880.0000 - val_mae: 2033.0277\n",
      "Epoch 97/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20813884.0000 - mse: 20813884.0000 - mae: 2090.8728 - val_loss: 17017350.0000 - val_mse: 17017350.0000 - val_mae: 1911.2218\n",
      "Epoch 98/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20793202.0000 - mse: 20793202.0000 - mae: 2086.9028 - val_loss: 17026566.0000 - val_mse: 17026566.0000 - val_mae: 1931.5625\n",
      "Epoch 99/100\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 20826692.0000 - mse: 20826692.0000 - mae: 2100.1243 - val_loss: 17033472.0000 - val_mse: 17033472.0000 - val_mae: 1943.5236\n",
      "Epoch 100/100\n",
      "109/109 [==============================] - 0s 3ms/step - loss: 20779300.0000 - mse: 20779300.0000 - mae: 2081.0332 - val_loss: 17045866.0000 - val_mse: 17045866.0000 - val_mae: 1963.9302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ad3333700>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to improve the model by changing the architecture a little bit\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(7,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size = 100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80c32625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step - loss: 29535660.0000 - mse: 29535660.0000 - mae: 2246.5791\n"
     ]
    }
   ],
   "source": [
    "# Performs a little bit better\n",
    "test_loss, test_mse, test_mae = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a9b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
